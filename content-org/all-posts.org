#+author: Rohit Goswami

#+hugo_base_dir: ../
#+hugo_front_matter_format: yaml
#+hugo_front_matter_key_replace: description>summary
#+bibliography: biblio/refs.bib

#+seq_todo: TODO DRAFT DONE
#+seq_todo: TEST__TODO | TEST__DONE

#+property: header-args :eval never-export

#+startup: logdone indent overview inlineimages

* DONE About
:PROPERTIES:
:EXPORT_HUGO_SECTION: /
:EXPORT_FILE_NAME: about
:EXPORT_DATE: 1995-08-10
:END:

#+begin_description
A short historical recollection of some *thoughts* and _stuff_.
#+end_description

*Hi.*

I'm [[https://orcid.org/0000-0002-2393-8056][Rohit Goswami]], better known across the web as ~HaoZeke~. I'm not the
first of my name, which is why instead of ~rgoswami~, I occasionally use ~rg0swami~ when I need to be
identified by something closer to my name.

The actual username is a throwback to back when people *liked* being anonymous (and with multiple personalities)
online, so that ought to give an idea of how old I am. A curriculum vitae is
[[https://github.com/HaoZeke/CV/blob/master/RG_Latest-cv.pdf][available here]].

It is difficult to keep this section short and not let it spill
into an unstructured memoir. For a while I considered trying to consolidate my
online presences but that turned out to be completely impossible without a
series of posts and avatars[fn:notrefs]. I did however eventually set up a
sporadically updated [[Collection of WebLinks][collection of web-links]] involving me.
** Intangible Positions
This is a set of things which are primarily online and/or voluntary in a
non-academic sense.
- I administer and design a bunch of websites, mostly verified [[https://keybase.io/HaoZeke][on Keybase]]
- I am a certified Software Carpentries [[https://static.carpentries.org/instructors#HaoZeke][instructor]]
- I [[https://static.carpentries.org/maintainers/#HaoZeke][officially maintain]], for the Software Carpentries, the lesson on R (r-novice-inflammation)
- I [[https://aur.archlinux.org/packages/?SeB=m&K=HaoZeke][also maintain]] some packages on the AUR (ArchLinux User Repository)
- I hone coursework development and teaching [[https://www.univ.ai/teams/rohit-goswami][with univ.ai]]
- I [[https://forum.xda-developers.com/xperia-z5/orig-development/cm-14-1-lineageos-t3536846][maintain(ed)]] the official LineageOS image for the Xperia Z5 Dual
** Historical Places
What follows is a more informal set of places I am or have been associated with or are of significance to
me[fn:growingUp].
*** Reykjav√≠k
- I [[https://english.hi.is/staff/rog32][am associated]] with the [[https://notendur.hi.is/hj/researchgroup.html][reputed Jonsson group]] of the [[http://raunvisindastofnun.hi.is/the_science_institute][Science Institute]] at the
  [[https://english.hi.is/school_of_engineering_and_natural_sciences][University of Iceland]], where I benefit from the
  guidance of the erudite and inspiring [[https://notendur.hi.is/hj/indexE.html][Prof. Hannes Jonsson]]
- My doctoral committee is here, which includes the very excellent inputs of
  [[https://english.hi.is/staff/elvarorn][Dr. Elvar Jonsson]]
- I have also benefited from sitting in on some formal coursework here, which
  has been a fascinatingly useful experience
*** Kanpur
- I [[https://femtolab.science/people/rohit][retain a close association]] with the fantastic [[https://femtolab.science/][Femtolab]] at [[http://home.iitk.ac.in/~dgoswami/][IIT Kanpur]] under
  [[https://femtolab.science/people/dgoswami][Prof. Debabrata Goswami]], who has provided constant guidance throughout my career
- I am the co-lead developer of the FOSS scientific [[https://dseams.info][d-SEAMS software suite]] for
  [[https://wiki.dseams.info/#citation][graph theoretic approaches to structure determination]] of molecular dynamics
  simulations, along with my exceptional co-lead [[https://www.researchgate.net/profile/Amrita_Goswami2][Amrita Goswami]] of the CNS Lab
  under Prof. Jayant K. Singh at IITK
- I worked with the Nair group as part of the [[http://surge.iitk.ac.in/AnnualReport/report2017.pdf][Summer Undergraduate in Research Excellence]] (SURGE) program, also at IITK
- Harcourt Butler Technical Institute (HBTI) Kanpur, or the [[http://hbtu.ac.in/][Harcourt Butler Technological University]], as it is now called, was where I trained to be a chemical engineer
*** Bombay
- I [[https://rajarshichakrabarti.wixsite.com/rajarshichakrabarti/team][spent a formative summer]] under [[https://rajarshichakrabarti.wixsite.com/rajarshichakrabarti][Prof. Rajarshi Chakrabarti]] of the IIT Bombay
  Chemistry department, who has been instrumental in developing my interests
- I also spent some time discussing experiments with [[https://www.che.iitb.ac.in/online/faculty/rajdip-bandyopadhyaya][Prof. Rajdip Bandyopadhyaya]]
  of the IIT Bombay Chemical Engineering department during an industrial
  internship in fragnance compounding at the R&D department of KEVA Ltd. under
  [[https://in.linkedin.com/in/debojit-chakrabarty-b9a2262][Dr. Debojit Chakrabarty]]
*** Bangalore
- At IISc, I had the good fortune to meet Prof. Hannes Jonsson at a summer
  workshop [[https://chemeng.iisc.ac.in/rare-events/index.html][on Rare events]]
- At the [[http://bangaloreinternationalcentre.org/][BIC]], I undertook formal machine learning and artificial intelligence
  training under Harvard's [[https://www.extension.harvard.edu/faculty-directory/rahul-dave][Dr. Rahul Dave]] and [[https://iacs.seas.harvard.edu/people/pavlos-protopapas][Dr. Pavlos Protopapas]] as part of the [[https://univ.ai][univ.ai]]
  summer course
*** Chennai
- I spent a very fruitful summer on quantum tomography under [[https://www.imsc.res.in/~sibasish/qis.html][Prof. Sibashish Ghosh]] at the [[https://www.imsc.res.in/][Institute for Mathematical Sciences]] (IMSc Chennai)
** Avatars
I thought it might be of use to list a few of my more official visages. This is
mostly to ensure people do not confuse me with a Sasquatch[fn:notpersonal].
These mugshots are exactly that, mugshots for profile icons[fn:mountaintapir].

#+caption: A collage of mugshots, shuffled and not ordered by date to confuse people trying to kill me
[[file:images/avatarCollage.jpg]]

** Donations
If you've gotten this far, you might also want to check out the
following[fn:patreon]:
- [[https://www.patreon.com/rgoswami][Patreon]]
- [[https://liberapay.com/rohit][Librepay]]

[fn:patreon] There won't ever be any content behind paywalls though
[fn:growingUp] I grew up on the verdant and beautiful [[https://www.tifr.res.in/][TIFR Mumbai]] campus, and
completed high school and undergraduate stuff while playing with peacocks and things on
the [[https://www.iitk.ac.in][IIT Kanpur]] campus
[fn:notrefs] I didn't think it would be necessary, but just in case it isn't
clear, people listed here are not necessarily all references or anything, this is
a personal list of people associated with each city, not a cover letter
[fn:notpersonal] This is not a replacement for [[https://www.instagram.com/rg0swami/][an Instagram feed]] or a [[https://www.facebook.com/rg0swami][Facebook
wall]], or even a [[https://www.researchgate.net/profile/Rohit_Goswami2][ResearchGate]] or [[https://publons.com/researcher/2911170/rohit-goswami/][Publons]] or [[https://orcid.org/0000-0002-2393-8056][ORCID]] page; all of which I do sporadically remember I have
[fn:mountaintapir] Made with the [[https://github.com/tttppp/mountain_tapir][Mountain Tapir Collage Maker]]
* DONE Collection of WebLinks :@personal:ramblings:
CLOSED: [1995-08-10]
:PROPERTIES:
:EXPORT_FILE_NAME: rg-collection-weblinks
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments false
:END:
#+BEGIN_QUOTE
An attempt to re-claim and verify my digital presence.
#+END_QUOTE
** Background
I mentioned [[About][on my about page]], that it is nigh impossible to keep track of every
digital trace there is of me. That said, it is really not even a countable
infinite set yet, so it is a good idea to get started before it gets much worse.
This is minimally curated, and will only be sporadically updated, so take
everything here with a grain of salt. I honestly have no idea why anyone who is
not me would like to see this, other than to prove one of these with respect to
the rest[fn:whereswaldo].
** Profiles
*** Professional
- [[https://english.hi.is/staff/rog32][University of Iceland]]
- [[https://femtolab.science/people/rohit][FemtoLab]]
*** Voluntary
- The Carpentries
  - [[https://carpentries.org/instructors#HaoZeke][Instructor]], [[https://carpentries.org/maintainers#HaoZeke][Maintainer]]
- [[https://forum.igdore.org/t/rohit-goswami/656][IGDORE]]
- [[https://www.univ.ai/teams/rohit-goswami][Univ.ai]]
*** Academic
- [[https://publons.com/researcher/2911170/rohit-goswami/][Publons]]
- [[https://peerj.com/rgoswami/][PeerJ]]
- [[http://scholar.google.com/citations?user=36gIdJMAAAAJ&hl=en][Google Scholar]]
- [[https://orcid.org/0000-0002-2393-8056][OrCiD]]
- [[https://loop.frontiersin.org/people/829611/overview][Loop]]
- [[https://osf.io/c47v3/][OSF]]
*** Societies
- [[https://engage.aiche.org/network/community-directory/profile?UserKey=8f135c8b-747d-4f79-95fd-84975458e3bd][American Institute of Chemical Engineers]]
- [[https://ieeexplore.ieee.org/author/37086865956][IEEE]]
*** Communities
- [[https://figshare.com/authors/Rohit_Goswami/5453063][Figshare]]
*** Misc
- [[https://github.com/HaoZeke][Github]]
- [[https://gitlab.com/HaoZeke][Gitlab]]
- [[https://www.goodreads.com/user/show/33462912-rohit-goswami][Goodreads]]
- [[https://keybase.io/HaoZeke][Keybase]]
** Pages and Articles
*** By Me
- Everything on [[https://keybase.io/HaoZeke][any of my many websites]]
- [[https://www.aiche.org/community/sites/committees/young-professionals/blog/fundamental-research-chemical-engineering-undergrad][Write-up on research with a ChemE undergraduate degree]] for the American Institute of Chemical Engineers (AIChE) Young Professionals Committee (YPC)
- Hackernoon article on [[https://hackernoon.com/locking-and-encrypting-apps-with-encfs-c1484e77f479][Locking and Encrypting Apps with Encfs]]
*** Mentioning Me
**** Lists
- [[https://www.cirm-math.fr/Listes/liste_pre_verif.php?id_renc=2146&num_semaine=0][Mathematical Methods of Modern Statistics 2]] ([[https://www.cirm-math.com/cirm-virtual-event-2146.html][info]])
- [[https://chemeng.iisc.ac.in/rare-events/gallery.html][Gallery of RARE 2019]] at IISc
- [[https://gitlab.com/openresearchlabs/probabilistic_data_analysis_2020/-/blob/master/session7.md][Probablistic Data Analysis]] (University of Turku)
**** Teaching
- [[https://smcclatchy.github.io/2020-06-22-biotechPartners-pm/][Data Carpentry Ecology for Biotech Partners]] :: June 22-July 2, 2020 (instructor)
- [[https://sadilar.github.io/2020-06-29-SA-ONLINE/][Online Data Carpentry Workshop
SADiLaR, South Africa]] :: 29 June - 3 July, 2020 (instructor)
- [[https://coderefinery.github.io/2020-05-25-online/][2020 MegaCodeRefinery]] (helper)
**** Quotes
- Quoted in a [[https://www.stanforddaily.com/2020/06/08/code-in-place-makes-cs-accessible-to-thousands-worldwide/][Stanford Daily Article on CS106A Code in Place]]
- [[https://fortran-lang.org/newsletter/2020/06/01/Fortran-Newsletter-June-2020/][Fortran Monthly Newsletter (June 2020)]]
- Emacs News
  - [[https://sachachua.com/blog/2020/05/2020-05-11-emacs-news/][2020-06-22]] :: for [[Temporary LaTeX Documents with Orgmode][Temporary LaTeX Documents with Orgmode]]
  - [[https://sachachua.com/blog/2020/05/2020-05-11-emacs-news/][2020-06-15]] :: for [[Emacs for Nix-R][Emacs for Nix-R]]
  - [[https://sachachua.com/blog/2020/05/2020-05-11-emacs-news/][2020-05-11]] :: for [[An Orgmode Note Workflow][An Orgmode Note Workflow]]
  - [[https://sachachua.com/blog/2020/05/2020-05-04-emacs-news/][2020-05-04]] :: for [[Pandoc to Orgmode with Babel][Pandoc to Orgmode with Babel]]
  - [[https://sachachua.com/blog/2020/04/2020-04-27-emacs-news/][2020-04-27]] :: for [[Using Mathematica with Orgmode][Using Mathematica with Orgmode]]
  - [[https://sachachua.com/blog/2020/04/2020-04-13-emacs-news/][2020-04-13]] :: for [[https://dotdoom.rgoswami.me/][my dotDoom doom-emacs configuration]]
  - [[https://sachachua.com/blog/2020/04/2020-04-06-emacs-news/][2020-04-06]] :: for [[Replacing Jupyter with Orgmode][Replacing Jupyter with Orgmode]]
** Videos
*** Of Me
- Everything [[https://www.youtube.com/channel/UC9f_UGqNsa60kmNGj_WkLPw?][on my YouTube channel]]
- [[https://www.youtube.com/watch?v=Q1St1VT43sc&feature=youtu.be][Discussion session]] for the CS196A Code in Place AMA with the students

*** Including Me
This category involves recordings where I asked questions, and therefore
technically involve me in a sense.
- Code in Place AMA with [[https://www.youtube.com/watch?v=J7S_SJ2Adi4&list=PLcil-m27rjnZ2f85ao8_EauguYhzkzSQA&index=5&t=0s][Stanford CS Lecturers]]
**** Fortran Maintainers Monthly Calls
- [[https://www.youtube.com/watch?v=i-gRNGRzugc][June 2020]]
**** IAS TML Lecture Questions
I've been sitting in on these for a while thanks to Ke Li, but this section lists the lectures I asked a question in
  - "[[https://video.ias.edu/tml/2020/0609-AleksanderMadry][What Do Models Learn?]]" by Aleksander MƒÖdry
  - "[[https://video.ias.edu/machinelearning/2020/0611-MichaelI.Jordan][Langevin Dynamics in Machine Learning]]" by Michael Jordan

[fn:whereswaldo] If you do think you have seen me somewhere not on this list, drop me an email
* DONE Search
:PROPERTIES:
:EXPORT_HUGO_SECTION: /
:EXPORT_FILE_NAME: search
:END:
#+begin_src yaml :front_matter_extra t
layout: "search"
outputs:
  - html
  - json
sitemap:
  priority: 0.1
#+end_src
#+begin_description
Search in full-text, the entire contents of the site.
#+end_description
* DONE Categories
:PROPERTIES:
:EXPORT_HUGO_SECTION: /categories/
:EXPORT_FILE_NAME: _index.md
:END:
#+begin_src yaml :front_matter_extra t
mainlist: True
#+end_src
* DONE Tags
:PROPERTIES:
:EXPORT_HUGO_SECTION: /tags/
:EXPORT_FILE_NAME: _index.md
:END:
#+begin_src yaml :front_matter_extra t
mainlist: True
#+end_src
* DONE Site Rationale :@personal:ramblings:explanations:
:PROPERTIES:
:EXPORT_FILE_NAME: rationale
:EXPORT_DATE: 2020-02-11 23:28
:END:
** Why this site exists
I have a lot of online presences. I have been around (or at-least, lurking) for
over ten years. Almost as long as I have been programming. Anyway, I have a
penchant lately for using ~emacs~ and honestly there isn't very good support for
~org-mode~ files. There are options recently with ~gatsby~ as well, but this
seemed kinda neat.
** What 'this' is
- This site is [[http://gohugo.io/][built by Hugo]]
- The posts are [[https://ox-hugo.scripter.co/][generated with ox-hugo]]
- The theme is based of this [[https://github.com/rhazdon/hugo-theme-hello-friend-ng][excellent one]] by Djordje Atlialp, which in turn is based off of this [[https://github.com/panr/hugo-theme-hello-friend][theme by panr]]
    - My modifications [[https://github.com/HaoZeke/hugo-theme-hello-friend-ng-hz][are here]]
** What is here
- Mostly random thoughts I don't mind people knowing
- Some tech stuff which isn't coherent enough to be put in any form with
  references
- Emacs specific workflows which I might want to write about more than [[https://dotdoom.grimoire.science/][short
  notes on the config]]
** What isn't here
- Some collections should and will go to my [[https://grimoire.science][grimoire]]
- My [[https://dotdoom.rgoswami.me][doom-emacs configuration]]
- Academic stuff is better tracked on [[https://publons.com/researcher/2911170/rohit-goswami/][Publons]] or [[https://scholar.google.co.in/citations?user=36gIdJMAAAAJ&hl=en][Google Scholar]] or my pages
  hosted by my favorite [[https://femtolab.science/people/rohit][IITK group]] or [[https://www.hi.is/starfsfolk/rog32][UI group]]
* DONE Taming Github Notifications :@notes:tools:github:workflow:
:PROPERTIES:
:EXPORT_FILE_NAME: ghNotif
:EXPORT_DATE: 2020-02-12 11:36
:END:
** Background
As a member of several large organizations, I get a lot of github notifications.
Not all of these are of relevance to me. This is especially true of
~psuedo-monorepo~ style repositories like the [[https://github.com/openjournals/joss-reviews][JOSS review system]] and
*especially* the [[https://github.com/exercism/v3/][exercism community]].

- I recently (re-)joined the [[https://exercism.io/][exercism community]] as a maintainer for the C++
  lessons after having been a (sporadic) teacher
- This was largely in response to a community call to action as the group needed
  new blood to usher in *v3* of the exercism project

Anyway, I have since found that at the small cost of possibly much of my public
repo data, I can manage my notifications better with [[https://octobox.io/][Octobox]]

** Octobox
- It appears to be free for now
- It syncs on demand (useful)
- I can search things quite easily
- They have a neat logo
- There appear to be many features I probably won't use

It looks like this:

#+caption: Octobox Stock Photo
[[file:images/octoboxSample.png]]
* DONE Poetry and Direnv :@programming:tools:direnv:workflow:python:
:PROPERTIES:
:EXPORT_FILE_NAME: poetry-direnv
:EXPORT_DATE: 2020-02-13 21:36
:END:
** Background
- I end up writing about using [[https://python-poetry.org/][poetry]] a lot
- I almost always [[https://direnv.net/][use direnv]] in real life too
- I don't keep writing mini scripts in my ~.envrc~

Honestly there's nothing here anyone using the [[https://github.com/direnv/direnv/wiki/Python][direnv wiki]] will find surprising,
but then it is still neat to link back to.

** Setting Up Poetry
This essentially works by simply modifying the global ~.direnvrc~ which
essentially gets sourced by every local ~.envrc~ anyway.
#+BEGIN_SRC sh
vim $HOME/.direnvrc
#+END_SRC
So what we put in there is the following snippet derived from other snippets [[https://github.com/direnv/direnv/wiki/Python][on
the wiki]], and is actually now there too.

#+BEGIN_SRC bash
# PUT this here
layout_poetry() {
  if [[ ! -f pyproject.toml ]]; then
    log_error 'No pyproject.toml found.  Use `poetry new` or `poetry init` to create one first.'
    exit 2
  fi

  local VENV=$(dirname $(poetry run which python))
  export VIRTUAL_ENV=$(echo "$VENV" | rev | cut -d'/' -f2- | rev)
  export POETRY_ACTIVE=1
  PATH_add "$VENV"
}
#+END_SRC

Now we can just make ~.envrc~ files with ~layout_poetry~ and everything will
/just work‚Ñ¢/.

* DONE Replacing Jupyter with Orgmode :@programming:tools:emacs:workflow:orgmode:
:PROPERTIES:
:EXPORT_FILE_NAME: jupyter-orgmode
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:EXPORT_DATE: 2020-02-13 22:36
:END:
** Background
- I dislike Jupyter notebooks (and [[https://jupyter.org/][JupyterHub]]) a lot
- [[https://tkf.github.io/emacs-ipython-notebook/][EIN]] is really not much of a solution either

In the past I have written some posts on [[https://grimoire.science/latex-and-jupyterhub/][TeX with JupyterHub]] and discussed ways
to use virtual [[https://grimoire.science/python-and-jupyterhub/][Python with JupyterHub]] in a more reasonable manner.

However, I personally found that EIN was a huge pain to work with, and I mostly
ended up working with the web-interface anyway.

It is a bit redundant to do so, given that at-least for my purposes, the end
result was a LaTeX document. Breaking down the rest of my requirements went a
bit like this:

- What exports well to TeX? :: *Org*, Markdown, anything which goes into pandoc
- What displays code really well? :: LaTeX, Markdown, *Org*
- What allows easy visualization of code snippets? :: Rmarkdown, RStudio,
  JupyterHub, *Org* with babel

Clearly, [[https://orgmode.org/manual/][orgmode]] is the common denominator, and ergo, a perfect JupyterHub alternative.
** Setup
Throughout this post I will assume the following structure:
#+BEGIN_SRC bash :exports both
tree tmp
mkdir -p tmp/images
touch tmp/myFakeJupyter.org
#+END_SRC

#+RESULTS:
| tmp |                   |   |      |
| ‚îú‚îÄ‚îÄ | images            |   |      |
| ‚îî‚îÄ‚îÄ | myFakeJupyter.org |   |      |
| 1   | directory,        | 1 | file |

As is evident, we have a folder ~tmp~ which will have all the things we need for
dealing with our setup.

*** Virtual Python
Without waxing too eloquent on the whole reason behind doing this, since I will
rant about virtual python management systems elsewhere, here I will simply
describe my preferred method, which is [[https://python-poetry.org/][using poetry]].

#+BEGIN_SRC bash
# In a folder above tmp
poetry init
poetry add numpy matplotlib scipy pandas
#+END_SRC

The next part is optional, but a good idea if you figure out [[https://direnv.net/][using direnv]] and
have configured ~layout_poetry~ as [[https://rgoswami.me/posts/poetry-direnv][described here]]:
#+BEGIN_SRC bash
# Same place as the poetry files
echo "layout_poetry()" >> .envrc
#+END_SRC

*Note:*
- We can nest an arbitrary number of the ~tmp~ structures under a single place
  we define the poetry setup
- I prefer using ~direnv~ to ensure that I never forget to hook into the right environment
** Orgmode
This is not an introduction to org, however in particular, there are some basic
settings to keep in mind to make sure the set-up works as expected.

*** Indentation
Python is notoriously weird about whitespace, so we will ensure that our export
process does not mangle whitespace and offend the python interpreter. We will
have the following line at the top of our ~orgmode~ file:

#+BEGIN_SRC orgmode :tangle tmp/myFakeJupyter.org :exports code
# -*- org-src-preserve-indentation: t; org-edit-src-content: 0; -*-
#+END_SRC

*Note:*
- this post is actually generating the file being discussed here by
[[https://orgmode.org/manual/Extracting-Source-Code.html][tangling the file]]
- You can get the [[https://github.com/HaoZeke/haozeke.github.io/blob/src/content-org/tmp/myFakeJupyter.org][whole file here]]
*** TeX Settings
These are also basically optional, but at the very least you will need the
following:

#+BEGIN_SRC orgmode :tangle tmp/myFakeJupyter.org
#+author: Rohit Goswami
#+title: Whatever
#+subtitle: Wittier line about whatever
#+date: \today
#+OPTIONS: toc:nil
#+END_SRC

I actually use a lot of math using the ~TeX~ input mode in Emacs, so I like the
following settings for math:

#+BEGIN_SRC orgmode :tangle tmp/myFakeJupyter.org
# For math display
#+LATEX_HEADER: \usepackage{amsfonts}
#+LATEX_HEADER: \usepackage{unicode-math}
#+END_SRC

There are a bunch of other settings which may be used, but these are the bare
minimum, more on that would be in a snippet anyway.

*Note:*
- rendering math in the ~orgmode~ file in this manner requires that we
 use ~XeTeX~ to compile the final file
*** Org-Python
We essentially need to ensure that:
- Babel uses our virtual python
- The same session is used for each block

We will get our poetry python pretty easily:
#+BEGIN_SRC bash
which python
#+END_SRC

#+RESULTS:
: /home/haozeke/.cache/pypoetry/virtualenvs/test-2aLV_5DQ-py3.8/bin/python

Now we will use this as a common ~header-arg~ passed into the property drawer to
make sure we don't need to set them in every code block.

We can use the following structure in our file:

#+BEGIN_SRC orgmode :tangle tmp/myFakeJupyter.org :exports code
\* Python Stuff
  :PROPERTIES:
  :header-args:    :python /home/haozeke/.cache/pypoetry/virtualenvs/test-2aLV_5DQ-py3.8/bin/python :session One :results output :exports both
  :END:
Now we can simply work with code as we normally would
\#+BEGIN_SRC python
print("Hello World")
\#+END_SRC
#+END_SRC

*Note:*
- For some reason, this property needs to be set on *every* heading (as of Feb 13 2020)
- In the actual file you will want to remove extraneous ¬†\ symbols:
  - \* ‚Üí *
  - \#+BEGIN_SRC ‚Üí #+BEGIN_SRC
  - \#+END_SRC ‚Üí #+END_SRC
*** Python Images and Orgmode
To view images in ~orgmode~ as we would in a JupyterLab notebook, we will use a
slight trick.
- We will ensure that the code block returns a file object with the arguments
- The code block should end with a print statement to actually generate the file
  name

 So we want a code block like this:

#+begin_example
#+BEGIN_SRC python :results output file :exports both
import matplotlib.pyplot as plt
from sklearn.datasets.samples_generator import make_circles
X, y = make_circles(100, factor=.1, noise=.1)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plt.xlabel('x1')
plt.ylabel('x2')
plt.savefig('images/plotCircles.png', dpi = 300)
print('images/plotCircles.png') # return filename to org-mode
#+end_src
#+end_example

Which would give the following when executed:

#+begin_example
#+RESULTS:
[[file:images/plotCircles.png]]
#+end_example

Since that looks pretty ugly, this will actually look like this:

#+BEGIN_SRC python :results output file :exports both
import matplotlib.pyplot as plt
from sklearn.datasets.samples_generator import make_circles
X, y = make_circles(100, factor=.1, noise=.1)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plt.xlabel('x1')
plt.ylabel('x2')
plt.savefig('images/plotCircles.png', dpi = 300)
print('images/plotCircles.png') # return filename to org-mode
#+end_src

[[file:tmp/images/plotCircles.png]]

*** Bonus
A better way to simulate standard ~jupyter~ workflows is to just specify the
properties once at the beginning.

#+BEGIN_SRC orgmode
#+PROPERTY: header-args:python :python /home/haozeke/.cache/pypoetry/virtualenvs/test-2aLV_5DQ-py3.8/bin/python :session One :results output :exports both
#+END_SRC

This setup circumvents having to set the properties per sub-tree, though for
very large projects, it is useful to use different processes.
** Conclusions
- The last step is of course to export the file as to a ~TeX~ file and then
  compile that with something like ~latexmk -pdfxe -shell-escape file.tex~

There are a million and one variations of this of course, but this is enough to
get started.

The whole file is also [[https://github.com/HaoZeke/haozeke.github.io/blob/src/content-org/tmp/myFakeJupyter.org][reproduced here]].
* TODO Orgmode and Hugo :@programming:tools:emacs:webdev:hugo:
:PROPERTIES:
:EXPORT_FILE_NAME: hugo-orgmode
:END:
** Background
- This is about the site you are reading
- It is also a partial rant
- It has a lot to do with web development in general
* DONE Switching to Colemak :@personal:workflow:explanations:
:PROPERTIES:
:EXPORT_FILE_NAME: colemak-switch
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:EXPORT_DATE: 2020-02-29 14:06
:EXPORT_HUGO_AUTO_SET_LASTMOD: t
:END:
** Background
I just realized that it has been over two years since I switched from QWERTY to
Colemak but somehow never managed to write about it. It was a major change in my
life, and it took forever to get acclimatized to. I do not think I'll ever again be
in a position to make such a change in my life again, but it was definitely
worth it.
** Touch Typing
My interest in touch typing in I decided to digitize my notes for posterity, during the
last two years of my undergraduate studies back in Harcourt Butler Technical
Institute (HBTI) Kanpur, India. in one of my many instances of yak shaving, I
realized I could probably consume and annotate a lot more content by typing
faster. Given that at that stage I was already a fast talker, it seemed like a
natural extension. There was probably an element of nostalgia involved as well.
That and the end of a bachelors involves the thesis, which generally involves a
lot of typing.

There were (and are) some fantastic resources for learning to touch type
nowadays, I personally used:
- [[https://www.typing.com/][Typing.com]] :: This is short, but a pretty good basic setup. The numbering and
  special characters are a bit much to take in at the level of practice you get
  by completing all the previous exercises, but eventually they make for a good workout.
- [[https://www.typingclub.com/en-gb/login/][TypingClub]] :: This is what I ended up working my way through. It is
  comprehensive, beautiful, and fun.

Also, later, I ended up using [[https://www.keybr.com/][keybr]] a lot, simply because typing gibberish is a
good way of practicing, and it is independent of the keyboard layout.

Just to foreshadow things, the enemy facing me at this point was the layout
itself[fn:img] .

https://www.keyboard-design.com/kb-images/qwerty-kla.jpg

** Alternate layouts
Having finally broken into the giddy regimes of 150+ wpm, I was ecstatic, and
decided to start working my way through some longer reports. However, I quickly
realized I was unable to type for more than a couple of minutes without getting
terribly cramped. Once it got to the point of having to visit a physiotherapist,
I had to call it quits. At that stage, relearning the entire touch typing
corpus, given that I already was used to QWERTY, seemed pretty bleak.

It took forever, and I ended up applying my choices to my phone keyboard as
well, which presumably helped me in terms of increasing familiarity, had the
unintended effect of making me seem distant to people I was close to, since my
verbose texts suddenly devolved to painful one-liners.

The alternative layouts I tried were:

- [[https://www.dvorak-keyboard.com/][DVORAK]] :: At the time, TypingClub only supported QWERTY and DVORAK, so it was
  pretty natural for me to try it out. There are also some [[https://www.dvzine.org/][very nice comics
  about it]]. I remember that it was pretty neat, with
  a good even distribution, until I tried coding. The placement of the
  semicolons make it impossible to use while programming. I would still say it
  makes for a comfortable layout, as long as special characters are not required.

https://www.keyboard-design.com/kb-images/dvorak-kla.jpg

- [[http://mkweb.bcgsc.ca/carpalx][CarpalX]] :: I experimented with the entire carpalx family, but I was unable to get
  used to it. I liked QFMLWY best. I do recommend reading the training methodology, especially if
  anyone is interested in numerical optimization in general. More importantly,
  though it was relatively easy to set up on my devices and operating systems,
  the fact that it wasn't natively supported meant a lot of grief whenever I
  inevitably had to use a public computer.

https://www.keyboard-design.com/kb-images/qgmlwy-kla.jpg

- Colemak :: Eventually I decided to go with [[https://colemak.com/][Colemak]], especially since it is
  widely available. Nothing is easier than ~setxkbmap us -variant colemak -option grp:alt_shift_toggle~ on public machines and it's easy on Windows as
  well. Colemak seems like a good compromise. I personally have not been able to
  reach the same speeds I managed with QWERTY, even after a year, but then
  again, I can be a lot more consistent, and it hurts less. Nowadays, Colemak
  has made its way onto most typing sites as well, including TypingClub

https://www.keyboard-design.com/kb-images/colemak-kla.jpg

*** What about VIM?
- DVORAK makes it impossible, so do most other layouts, but there are some
  tutorials purporting to help use vim movement with DVORAK
- Colemak isn't any better, but the fact of the matter is that once you know VIM
  on QWERTY, and have separately internalized colemak or something else, hitting
  keys is just hitting keys

+All that said, I still occasionally simply remap HJKL (QWERTY movement) to HNEI (Colemak analog) when it is feasible.+
*update:* I actually ended up refactoring my entire Dotfiles to use more Colemak native bindings, as described [[Refactoring Dotfiles For Colemak][in this post]].
** Conclusion
Changing layouts was a real struggle. Watching my WPM drop back to lower than
hunt and peck styles was pretty humiliating, especially since the reports kept
coming in, and more than once I switched to QWERTY. However, since then, I have
managed to stay on course. I guess if I think about it, it boils down to a few
scattered thoughts:
- Typing is kinda like running a marathon, knowing how it is done and doing it
  are two different things
- Tell *everyone*, so people can listen to you lament your reduced speed and not
  hate you for replying slowly
- Practice everyday, because, well, it works out in the long run, even when you
  plateau
- Alternate shifts! That's really something which should show up more in
  tutorials, especially for listicles, not changing the shifts will really hurt
- Try and get a mechanical keyboard (like the [[https://www.annepro.net/][Anne Pro 2]] or the [[https://www.coolermaster.com/catalog/peripheral/keyboards/masterkeys-pro-l-white/][Coolermaster Masterkeys]]), they're fun and easy to change layouts on

[fn:img] The images are [[https://www.keyboard-design.com/best-keyboard-layouts.html][from here]], where there's also an effort based metric
used to score keyboard layouts.
* TODO Replacing Rstudio with Emacs :@programming:tools:emacs:workflow:R:
:PROPERTIES:
:EXPORT_FILE_NAME: rstudio-emacs
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:EXPORT_DATE: 2020-02-15 04:38
:END:
** Background
RStudio is one of the best IDEs around, in that it is essentially a text editor
and terminal with some pretty printing and object viewing functionality. It is
really great, but it is also relatively resource intensive. It turns out that
thanks to Emacs ESS, it is possible to circumvent Rstudio completely in favor of
an Emacs-native workflow.
* TODO Role models and colleges
* TODO My current courses
* TODO Rude college admissions
* DONE Pandora and Proxychains :@personal:tools:workflow:
:PROPERTIES:
:EXPORT_FILE_NAME: pandora-proxychains
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :comments true
:EXPORT_DATE: 2020-02-15 05:28
:END:
** Background
- Pandora doesn't work outside the states
- I keep forgetting how to set-up ~proxychains~
** Proxychains
Technically this article [[https://github.com/rofl0r/proxychains-ng][expects proxychains-ng]], which seems to be the more
up-to-date fork of the original ~proxychains~.

1. Install ~proxychains-ng~
   #+BEGIN_SRC bash
# I am on archlinux..
sudo pacman -S proxychains-ng
   #+END_SRC
2. Copy the configuration to the ~$HOME~ directory
   #+BEGIN_SRC bash
cp /etc/proxychains.conf .
   #+END_SRC
3. Edit said configuration to add some US-based proxy

In my particular case, I don't keep the tor section enabled.
#+BEGIN_SRC bash :exports both :results raw
tail $HOME/proxychains.conf
#+END_SRC

#+RESULTS:
#+begin_example
#
#       proxy types: http, socks4, socks5
#        ( auth types supported: "basic"-http  "user/pass"-socks )
#
[ProxyList]
# add proxy here ...
# meanwile
# defaults set to "tor"
# socks4 	127.0.0.1 9050
#+end_example

I actually use [[https://windscribe.com][Windscribe]] for my VPN needs, and they have a neat [[https://windscribe.com/getconfig/socks][SOCKS5 proxy
setup]]. This works out to a line like ~socks5 $IP $PORT $USERNAME $PASS~ being
added. The default generator gives you a pretty server name, but to get the IP
I use ~ping $SERVER~ and put that in the ~conf~ file.
** Pandora
I use the excellent ~pianobar~ frontend.
1. Get [[https://github.com/PromyLOPh/pianobar][pianobar]]
   #+BEGIN_SRC bash
sudo pacman -S pianobar
   #+END_SRC
2. Use it with ~proxychains~
   #+BEGIN_SRC bash
proxychains pianobar
   #+END_SRC
3. Profit

I also like setting up some defaults to make life easier:
#+BEGIN_SRC bash
mkdir -p ~/.config/pianobar
vim ~/.config/pianobar/config
#+END_SRC
I normally set the following (inspired by the [[https://wiki.archlinux.org/index.php/Pianobar][ArchWiki]]):
#+BEGIN_SRC conf
audio_quality = {high, medium, low}
autostart_station = $ID
password = "$PASS"
user = "$emailID"
#+END_SRC

The ~autostart_station ID~ can be obtained by inspecting the terminal output
during an initial run. I usually set it to the QuickMix station.
* Bojack Horseman :@personal:thoughts:random:review:TV:
:PROPERTIES:
:EXPORT_FILE_NAME: bojack-horseman
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :comments false
:EXPORT_DATE: 2020-02-27 22:28
:END:
** Background
For a while I was worried about writing about a TV show here. I thought it might
be frivolous, or worse, might outweigh the other kinds of articles I would like
to write. However, like most things, that which is ignored just grows, so it is
easier to just write and forget about it.
** The Show
Much has been said about how Bojack Horseman is one of the best shows ever, and
they're all correct. For that matter I won't be going into the details of how
every episode ties together a tapestry of lives in a meaningful way, or any of
that. The show was amazingly poignant. The characters felt real. Which actually
leads me to the real issue.
** The End
The end of Bojack was *good*. It was the way it was meant to be. For a
slice-of-life show, it is a natural conclusion. It isn't necessary that any
catharsis occurs or that the characters change or become better or all that
jazz. It isn't about giving the viewers closure. It is simply about a window
onto the lives of (fictional) characters being shut. To that end, I disliked
attempts to bring closure in the show itself.

One of the main reasons why I felt strongly enough to write this, is simply
because when I looked around, the prevailing opinion was that the main character
should have been killed off, _for his sins_. This strikes me as a very flippant
attitude to take. It reeks of people trying to make the show a cautionary tale,
which is frankly speaking a weird approach to take towards any fictional story.
The idea that the character should be redeemed also seemed equally weak, for
much the same reasons.

The fact that the characters are hypocrites, and that none of them are as good
or bad as they make themselves out to be is one of the best parts of the show.

** Conclusion
That's actually all I have to say about this. I thought of adding relevant memes
or listing episodes or name dropping sites, but this isn't buzzfeed. The show is
incredible, and there are far better ways of proving that. Bust out your
favorite search engine + streaming content provider / digital piracy eye-patch
and give it a whirl. The only thing I'd suggest is watching everything in order,
it's just that kind of show.

* TODO The Morpho Language :@programming:review:
:PROPERTIES:
:EXPORT_FILE_NAME: morpho-lang
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:END:
* TODO Towards DOOM-Emacs :@programming:workflow:review:
:PROPERTIES:
:EXPORT_FILE_NAME: towards-doom-emacs
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments false
:END:
** Background
[[https://dotdoom.grimoire.science/][My doom-emacs configuration]] gets a rather insane number of views every month.
Statistically, it accounts for 90% of the traffic to [[https://grimoire.science][my other site]], and that is
essentially around three times time traffic on the rest of my presences,
combined. I followed a pretty standard path to finally reach doom-emacs.
However, before delving into it, I thought I'd discuss the chronological aspects
of my road to doom. In a nutshell it was just:

Word ‚Üí Notepad++ ‚Üí Sublime Text 3 ‚Üí VIM ‚Üí Emacs (Spacemacs) ‚Üí Emacs (doom-emacs)
* DONE Provisioning Dotfiles on an HPC :@programming:workflow:projects:hpc:
:PROPERTIES:
:EXPORT_FILE_NAME: prov-dots
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:EXPORT_DATE: 2020-03-16 00:06
:END:
** Background
[[https://github.com/HaoZeke/Dotfiles][My dotfiles]] turned 4 years old a few months ago (since 9th Jan 2017) and remains one of my most
frequently updated projects for obvious reasons. Going through the changes
reminds me of a whole of posts I never got around to writing.

Anyway, recently I gained access to another HPC cluster, with a standard configuration
(bash, old CentOS) and decided to track my provisioning steps. This is really a
very streamlined experience by now, since I've used the same setup across scores
of machines. This is actually also a generic intro to configuring user setups on
HPC (high performance cluster) machines, if one is inclined to read it in that
manner. To that end, sections of this post involve restrictions relating to user
privileges which aren't normally part of most Dotfile setups.
*** Aside
- Dotfiles define most people who maintain them
- No two sets are ever exactly alike
- They fall somewhere between winging it for each machine and using something
  like [[https://www.habitat.sh/learn/][Chef]] or [[https://www.ansible.com/][Ansible]]
- Tracking dotfiles is really close to having a sort of out-of-context journal

Before I settled on using [[https://github.com/kobus-v-schoor/dotgit][the fabulous dotgit]], I considered several
alternatives, most notably [[https://www.gnu.org/software/stow/][GNU stow]].
** Preliminaries
It is important to note the environment into which I had to get my
setup.
*** SSH Setup
- The very first thing to do is to use a new ~ssh-key~
#+BEGIN_SRC bash
export myKey="someName"
ssh-keygen -f $HOME/.ssh/$myKey
# I normally don't set a password
ssh-add $HOME/.ssh/$myKey
ssh-copy-id $myHPC
# myHPC being an IP address
#+END_SRC
I more often than not tend to back this up with a cutesy alias, also because I
do not always get my username of choice on these machines. So in
~$HOME/.ssh/config~ I use:
#+BEGIN_SRC conf
Host myHPC
 Hostname 127.0.0.1
 User somethingIgot
 IdentityFile ~/.ssh/myKey
#+END_SRC
*** Harvesting Information
- I normally use [[https://github.com/dylanaraps/neofetch][neofetch]] on new machines
#+BEGIN_SRC bash
mkdir -p $HOME/Git/Github
cd $HOME/Git/Github
git clone https://github.com/dylanaraps/neofetch.git
cd neofetch
./neofetch
#+END_SRC

#+caption: Neofetch Output
[[file:images/sampleHPC.png]]

Where the top has been tastefully truncated. Just for context, the latest ~bash~
as of this writing is ~v5.0.16~ so, that's not too bad, given that ~neofetch~
works for ~bash~ ‚â• 3.2

** TODO Circumventing User Restrictions with Nix
- A post in and of itself would be required to explain why and how users are
  normally restricted from activities in cluster nodes
- Here, we leverage the [[https://nixos.org/nix/manual/#chap-installation][nix-package management system]] to circumvent these
- User installation of ~nix~ is sadly non-trivial, so this might be of some use[fn:nixUsr]
*** Testing nix-user-chroot
1. We will first check namespace support
#+BEGIN_SRC bash
# Errored out
unshare --user --pid echo YES
# Worked!
zgrep CONFIG_USER_NS /boot/config-$(uname -r)
# CONFIG_USER_NS=y
#+END_SRC

Thankfully we have support for namespaces, so we can continue with ~nix-user-chroot~.

2. Since we definitely do not have ~rustup~ or ~rustc~ on the HPC, we will use [[https://github.com/nix-community/nix-user-chroot/releases][a
   prebuilt binary]] of ~nix-user-chroot~

#+BEGIN_SRC bash
cd $HOME && wget -O nix-user-chroot  https://github.com/nix-community/nix-user-chroot/releases/download/1.0.2/nix-user-chroot-bin-1.0.2-x86_64-unknown-linux-musl
#+END_SRC

3. Similar to [[https://nixos.wiki/wiki/Nix_Installation_Guide#Installing_without_root_permissions][the wiki example]], we will use ~$HOME/.nix~

#+BEGIN_SRC bash
cd ~/
chmod +x nix-user-chroot
mkdir -m 0755 ~/.nix
./nix-user-chroot ~/.nix bash -c 'curl https://nixos.org/nix/install | sh'
#+END_SRC

- Only, this *doesn't work*

Turns out that since ~unshare~ is too old, ~nix-user-chroot~ won't work either.

*** Using PRoot
PRoot is pretty neat in general, they even have a [[https://proot-me.github.io/][nice website describing it]].
0. Set a folder up for local installations (this is normally done by my
   Dotfiles, but we might as well have one here too)
#+BEGIN_SRC bash
mkdir -p $HOME/.local/bin
export PATH=$PATH:$HOME/.local/bin
#+END_SRC
1. Get a binary from the [[https://gitlab.com/proot/proot/-/jobs][GitLab artifacts]]
#+BEGIN_SRC bash
cd $HOME
mkdir tmp
cd tmp
wget -O artifacts.zip https://gitlab.com/proot/proot/-/jobs/452350181/artifacts/download
unzip artifacts.zip
mv dist/proot $HOME/.local/bin
#+END_SRC
2. Bind and install ~nix~
#+BEGIN_SRC bash
mkdir ~/.nix
export PROOT_NO_SECCOMP=1
proot -b ~/.nix:/nix
export PROOT_NO_SECCOMP=1
curl https://nixos.org/nix/install | sh
#+END_SRC

If you're very unlucky, like I was, you may be greeted by a lovely little error
message along the lines of:

#+begin_example
/nix/store/ddmmzn4ggz1f66lwxjy64n89864yj9w9-nix-2.3.3/bin/nix-store: /opt/ohpc/pub/compiler/gcc/5.4.0/lib64/libstdc++.so.6: version `GLIBCXX_3.4.22' not found (required by /nix/store/c0b76xh2za9r9r4b0g3iv4x2lkw1zzcn-aws-sdk-cpp-1.7.90/lib/libaws-cpp-sdk-core.so)
#+end_example

Which basically is as bad as it sounds. At this stage, we need a newer compiler
to even get ~nix~ up and running, but can't without getting an OS update. This
chicken and egg situation calls for the drastic measure of leveraging ~brew~
first[fn:brewStuff].

#+BEGIN_SRC bash
sh -c "$(curl -fsSL https://raw.githubusercontent.com/Linuxbrew/install/master/install.sh)"
#+END_SRC

Note that nothing in this section suggests the best way is not to lobby your
sys-admin to install ~nix~ system-wide in multi-user mode.
** Giving Up with Linuxbrew
- Somewhere around this point, [[https://docs.brew.sh/Homebrew-on-Linux][linuxbrew]] is a good idea
- More on this later
** Shell Stuff
~zsh~ is my shell of choice, and is what my ~Dotfiles~ expect and work best with.
- I did end up making a quick change to update the ~dotfiles~ with a target
  which includes a snippet to transition to ~zsh~ from the default ~bash~ shell
** Dotfiles
The actual installation steps basically tracks [[https://github.com/HaoZeke/Dotfiles][the readme instructions]].

#+BEGIN_SRC bash
git clone https://github.com/kobus-v-schoor/dotgit.git
mkdir -p ~/.bin
cp -r dotgit/bin/dotgit* ~/.bin
cat dotgit/bin/bash_completion >> ~/.bash_completion
rm -rf dotgit
# echo 'export PATH="$PATH:$HOME/.bin"' >> ~/.bashrc
echo 'export PATH="$PATH:$HOME/.bin"' >> ~/.zshrc
#+END_SRC

[fn:nixUsr] Much of this section is directly adapted from [[https://nixos.wiki/wiki/Nix_Installation_Guide#Installing_without_root_permissions][the NixOS wiki]]
[fn:brewStuff] This used to be called linuxbrew, but the [[https://docs.brew.sh/Homebrew-on-Linux][new site]] makes it clear
that it's all one ~brew~ now.
* DONE Shorter Posts :@notes:tools:rationale:workflow:ideas:
:PROPERTIES:
:EXPORT_FILE_NAME: shortpost
:EXPORT_DATE: 2020-03-16 00:16
:END:
** Background
Sometime this year, I realized that I no longer have access to a lot of my older
communication. This included, a lot of resources I enjoyed and shared with the
people who were around me at that point in time. To counter this, I have decided
to opt for shorter posts, even if they don't always include the same level of
detail I would prefer to provide.

*** Alternatives
- I have an automated system based around IFTTT combined with Twitter, Diigo,
  and even Pocket
- However, that doesn't really tell me much, and trawling through a massive glut
  of data is often pointless as well
- There's always Twitter, but I don't really care to hear the views of others
  when I want to revisit my own ideas
** Conclusions
- I will be making shorter posts here, like the random one on [[https://rgoswami.me/posts/ghnotif/][octobox]]
* DONE D3 for Git :@notes:tools:rationale:workflow:ideas:
:PROPERTIES:
:EXPORT_FILE_NAME: d3git
:EXPORT_DATE: 2020-03-16 00:17
:END:
** Background
- I have had a lot of discussions regarding the teaching of ~git~
- This is mostly as a part of [[https://static.carpentries.org/maintainers/#HaoZeke][the SoftwareCarpentries]], or in view of my
  [[https://www.univ.ai/teams/rohit-goswami][involvement with univ.ai]], or simply in every public space I am associated with
- Without getting into my views, I just wanted to keep this resource in mind
** The site
- Learning ~git~ is a highly contentious thing
- People seem to be fond of GUI tools, especially since on non *nix systems, it
  seems that there is a lot of debate surrounding obtaining the ~git~ utility in
  the first place

One of the best ways of understanding (without installing stuff) the mental
models required for working with ~git~ is [[https://onlywei.github.io/explain-git-with-d3/#checkout][this site]]

#+caption: A screenshot of the site
[[file:images/d3git.png]]

- However, as is clear, this is not exactly a replacement for a good old command-line.

- It does make for a good resource for teaching with slides, or for generating
  other static visualizations, where live coding is not an option
* DONE Trees and Bags :@notes:theory:statistics:math:
:PROPERTIES:
:EXPORT_FILE_NAME: trees-and-bags
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:EXPORT_HUGO_CUSTOM_FRONT_MATTER+: :link-citations true
:EXPORT_HUGO_PANDOC_CITATIONS: t
:EXPORT_DATE: 2020-03-26 00:28
:END:
# :EXPORT_HUGO_CUSTOM_FRONT_MATTER+: :nocite '(@hastieElementsStatisticalLearning2009)

#+BEGIN_QUOTE
  Explain why using bagging for prediction trees generally improves
  predictions over regular prediction trees.
#+END_QUOTE


** Introduction

Bagging (or Bootstrap Aggregation) is one of the most commonly used
ensemble method for improving the prediction of trees. We will broadly
follow a historical development trend to understand the process. That
is, we will begin by considering the Bootstrap method. This in turn
requires knowledge of the Jacknife method, which is understandable from
a simple bias variance perspective. Finally we will close out the
discussion by considering the utility and trade-offs of the Bagging
technique, and will draw attention to the fact that the Bagging method
was contrasted to another popular ensemble method, namely the Random
Forest method, in the previous section.

Before delving into the mathematics, recall that the approach taken by
bagging is given as per @cichoszDataMiningAlgorithms2015 to be:

- create base models with *bootstrap* samples of the training set
- combine models by unweighted voting (for classification) or by
  averaging (for regression)

The reason for covering the Jacknife method is to develop an intuition
relating to the sampling of data described in the following table:

| Data-set   Size  per   sample | Estimator         |
| Reduces                       | Jacknife          |
| Remains    the   same         | Bootstrap         |
| Increases                     | data-augmentation |

** Bias Variance Trade-offs

We will recall, for this discussion, the bias variance trade off which
is the basis of our model accuracy estimates (for regression) as per the
formulation of @jamesIntroductionStatisticalLearning2013.

\begin{equation}
E(y‚ÇÄ-\hat{f}(x‚ÇÄ))¬≤=\mathrm{Var}(\hat{f}(x‚ÇÄ))+[\mathrm{Bias(\hat{f(x‚ÇÄ)})}]¬≤+\mathrm{Var}(Œµ)
\end{equation}

Where:

- $E(y_{0}-\hat{f}(x_{0}))¬≤$ is the expected test MSE, or the average
  test MSE if $f$ is estimated with a large number of training sets and
  tested at each $x‚ÇÄ$
- The variance is the amount by which our approximation $\hat{f}$ will
  change if estimated by a different training set, or the *flexibility*
  error
- The bias is the (reducible) *approximation* error, caused by not
  fitting to the training set exactly
- $\mathrm{Var}(Œµ)$ is the *irreducible* error

We will also keep in mind, going forward the following requirements of a
good estimator:

- Low variance AND low bias
- Typically, the variance increases while the bias decreases as we use
  more flexible methods (i.e.¬†methods which fit the training set
  better[fn:smooth])

Also for the rest of this section, we will need to recall from
@hastieElementsStatisticalLearning2009, that the bias is given by:

\begin{equation}
[E(\hat{f_{k}}(x‚ÇÄ)-f(x‚ÇÄ)]¬≤
\end{equation}

Where the expectation averages over the randomness in the training data.

To keep things in perspective, recall from
@hastieElementsStatisticalLearning2009:

#+CAPTION: Test and training error as a function of model complexity
[[file:images/biasVar.png]]

** Jacknife Estimates
    :PROPERTIES:
    :CUSTOM_ID: jacknife-estimates
    :END:

We will model our discussion on the work of
@efronJackknifeBootstrapOther1982. Note that:

- The $\hat{Œ∏}$ symbol is an estimate of the true quantity $Œ∏$
- This is defined by the estimate being $\hat{Œ∏}=Œ∏(\hat{F})$
- $\hat{F}$ is the empirical probability distribution, defined by mass
  $1/n$ at $x·µ¢ ‚àÄ i‚ààI$, i is from 1 to n

The points above establishes our bias to be given by
$E_FŒ∏(\hat{F})-Œ∏(F)$ such that $E_F$ is the expectation under x‚ÇÅ‚ãØx‚Çô~F.

To derive the Jacknife estimate $(\tilde{Œ∏})$ we will simply
sequentially delete points x·µ¢ (changing $\hat{F}$), and recompute our
estimate $\hat{Œ∏}$, which then simplifies to:

\begin{equation}
\tilde{Œ∏}\equiv n\hat{Œ∏}-(\frac{n-1}{n})‚àë_{i=1}‚Åø\hat{Œ∏}
\end{equation}

In essence, the Jacknife estimate is obtained by making repeated
estimates on increasingly smaller data-sets. This intuition lets us
imagine a method which actually makes estimates on larger data-sets
(which is the motivation for data augmentation) or, perhaps not so
intuitively, on estimates on data-sets of the same size.

** Bootstrap Estimates

Continuing with the same notation, we will note that the bootstrap is
obtained by draw random data-sets with replacement from the training
data, where each sample is the same size as the original; as noted by @hastieElementsStatisticalLearning2009.

We will consider the bootstrap estimate for the standard deviation of
the $\hat{Œ∏}$ operator, which is denoted by $œÉ(F,n,\hat{\theta})=œÉ(F)$

The bootstrap is simple the standard deviation at the approximate F,
i.e., at $F=\hat{F}$:

\begin{equation}
\hat{\mathrm{SD}}=\sigma(\hat{F})
\end{equation}

Since we generally have no closed form analytical form for $œÉ(F)$ we
must use a Monte Carlo algorithm:

1. Fit a non parametric maximum likelihood estimate (MLE) of F,
   i.e.¬†$\hat{F}$
2. Draw a sample from $\hat{F}$ and calculate the estimate of $\hat{Œ∏}$
   on that sample, say, $\hat{Œ∏}^*$
3. Repeat 2 to get multiple (say B) replications of $\hat{Œ∏}^*$

Now we know that as $B‚Üí‚àû$ then our estimate would match $œÉ(\hat{F})$
perfectly, however, since that itself is an estimate of the value we are
actually interested in, in practice there is no real point using a very
high B value.

Note that in actual practice we simply use the given training data with
repetition and do not actually use an MLE of the approximate true
distribution to generate samples. This causes the bootstrap estimate to
be unreasonably good, since there is always significant overlap between
the training and test samples during the model fit. This is why cross
validation demands non-overlapping data partitions.

*** Connecting Estimates

The somewhat surprising result can be proved when $\hat{Œ∏}=Œ∏(\hat{F}$ is
a quadratic functional, namely:

\begin{equation}\hat{\mathrm{Bias}}_{boot}=\frac{n-1}{n} \hat{\mathrm{Bias}}_{jack}\end{equation}

In practice however, we will simply recall that the Jacknife tends to
overestimate, and the Bootstrap tends to underestimation.

** Bagging

Bagging, is motivated by using the bootstrap methodology to improve the
estimate or prediction directly, instead of using it as a method to
asses the accuracy of an estimate. It is a representative of the
so-called parallel ensemble methods where the base learners are
generated in parallel. As such, the motivation is to reduce the error by
exploiting the independence of base learners (true for mathematically
exact bootstrap samples, but not really true in practice).

Mathematically the formulation of @hastieElementsStatisticalLearning2009
establishes a connection between the Bayesian understanding of the
bootstrap mean as a posterior average, however, here we will use a more
heuristic approach.

We have noted above that the bagging process simply involves looking at
different samples in differing orders. This has some stark repercussions
for tree-based methods, since the trees are grown with a /greedy/
approach.

- Bootstrap samples may cause different trees to be produced
- This causes a reduction in the *variance*, especially when not too
  many samples are considered
- Averaging, reduces variance while leaving bias unchanged

Practically, these separate trees being averaged allows for varying
importance values of the variables to be calculated.

In particular, following @hastieElementsStatisticalLearning2009, it is
possible to see that the MSE tends to decrease by bagging.

\begin{align}
 E_P[Y-\hat{f}^*(x)]¬≤ & = & E_P[Y-f*{ag}(x)+f^*_{ag}(x)-\hat{f}^*(x)]¬≤ \\
& = & E_P[Y-f^*_{ag}(x)]¬≤+E_P[\hat{f}^*(x)-f^*_{ag}(x)]¬≤ ‚â• E_P[Y-f^*_{ag}(x)]¬≤
\end{align}

Where:

- The training observations are independently drawn from a distribution
  $P$
- $f_{ag}(x)=E_P\hat{f}^*(x)$ is the ideal aggregate estimator

For the formulation above, we assume that $f_{ag}$ is a true bagging
estimate, which draws samples from the actual population. The upper
bound is obtained from the variance of the $\hat{f}^*(x)$ around the
mean, $f_{ag}$

Practically, we should note the following:

- The regression trees are deep
- The greedy algorithm growing the trees cause them to be unstable
  (sensitive to changes in input data)
- Each tree has a high variance, and low bias
- Averaging these trees reduces the variance

Missing from the discussion above is how exactly the training and test
sets are used in a bagging algorithm, as well as an estimate for the
error for each base learner. This has been reported in the code above as
the OOB error, or out of bag error. We have, as noted by
@zhouEnsembleMethodsFoundations2012 and @breimanBaggingPredictors1996
the following considerations.

- Given $m$ training samples, the probability that the i·µó ∞ sample is
  selected 0,1,2... times is approximately Poisson distributed with
  $Œª=1$
- The probability of the i·µó ∞ example will occur at least once is then
  $1-(1/e)‚âà0.632$
- This means for each base learner, there are around $36.8$ % original
  training samples which have not been used in its training process

The goodness can thus be estimated using these OOB error, which is
simply an estimate of the error of the base tree on the OOB samples.

As a final note, random forests are conceptually easily understood by combining
bagging with subspace sampling, which is why in most cases and packages, we used
bagging as a special case of random forests, i.e.¬†when no subspace sampling is
performed, random forests algorithms perform bagging.


[fn:smooth] This is mostly true for reasonably smooth true functions
* TODO d-SEAMS got published
* TODO Why I don't cure cancer
With the coronavirus pandemic going on, some of the louder rabble of the
academic community (as evinced by Twitter) have been calling for the shut down
of non-essential work. The real reason why it doesn't matter if there's a
pandemic going on is simply because the work keeps you up anyway. Working on
projects you love is like carrying a pandemic around with you all the time. It
is impossible to let go of in the first place. Understandably not everyone works
like this, and there are as many reasons to be on a project as there are people
probably.
* TODO Machine Learning is not the future
- I dislike machine learning in terms of scientific achievement
- Competitions are no way to bring a field forward
- State of the art on a month to month basis is a very poor way of understanding
  any field
- The ability to provide direct industrial applications is probably why this is
  so popular

This kind of behavior would be pretty unthinkable in other fields. The push to
clear a benchmark simply discards the basic ideas behind learning a subject in
the first place.
* TODO The net is not for socializing
- I used to go online to be an idea, an embodiment of an idea
Nowadays we bring our selves to the internet and I don't think that is as
liberating as the older format.
* DONE Analytics: Google to Goat :@notes:tools:rationale:workflow:ideas:
CLOSED: [2020-04-09 Thu 17:17]
:PROPERTIES:
:EXPORT_FILE_NAME: goat-google
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments false
:EXPORT_DATE: 2020-04-09 11:17
:END:
** Background
Like a lot of my tech based rants, this was brought on by a recent [[https://news.ycombinator.com/item?id=22813168][Hacker News
post]]. I won't go into why the product listed there is a hollow faux FOSS
rip-off. I won't discuss how the 'free' analytics option, like many others are
just hobby projects taking pot shots at other projects. Or how insanely
overpriced most alternatives are.

I will however discuss why and how I transitioned to using the awesome Goat
Counter.
** Google Analytics
I would like to point out that it is OK to start out with Google Analytics. It
is easy, and free, and scales well. There are reasons not to, but it is a good
starting point.

*** Pros
- Google Analytics is free, truly free
- The metrics are very detailed
- It is easy to set up
*** Cons
- Privacy concerns
- Blocked by people
- Easy to obsess over metrics

** Goat Counter
As with most Hacker News posts, the article itself was nothing compared to the
excellent comment thread. It was there that I came across people praising [[https://www.goatcounter.com/][Goat Counter]].

*** Pros
- Is open sourced ([[https://github.com/zgoat/goatcounter][here on Github]])
- Super lightweight
- Anonymous statistics
- Easy to share
*** Cons
- Has an upper limit on free accounts (10k a month)
- I am not very fond of Go
** Conclusions
I might eventually go back to GA, if I go over the 10k page view limit. Then
again, I might not. It might be more like, I only care about the first 10k
people who make it to my site.

**UPDATE:** This site has since shifted to Clicky, for [[Analytics II: Goat to Clicky][reasons outlined here]]
* TODO Fediverse Thoughts
** Background
I recently decided to take a half day off. Naturally I began looking into things
I've never seen before. I then ran into the delightful fediverse again.
* DONE On-boarding for Code in Place :@notes:ideas:teaching:cs106a:
CLOSED: [2020-04-10 Fri 16:01]
:PROPERTIES:
:EXPORT_FILE_NAME: scp-onboarding
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:END:
** Background
A few weeks ago, I ended up recording a video for the [[https://compedu.stanford.edu/codeinplace/announcement/][Stanford CS106A: Code in Place]]
initiative (which can be [[https://youtu.be/J0ULMEtM00w][found here]]). I heard back a while ago, and am now to lead a section for the
course!

I'll probably be making a series of short posts as this process continues.
** On-Boarding
This was very reminiscent of the [[http://carpentries.github.io/instructor-training/][Carpentries instructor training]], which makes
sense, given how well thought out that experience was.

We started out with a pre-presentation where people were able to just spitball
and connect, which is pretty neat.

One of the interesting parts of this, was the idea of *interactive recorded
lectures*, where the professors will be watching lectures with the students. The
entire slide deck [[https://docs.google.com/presentation/d/12DFKzJWYunNbVMdJ3PbMlS3ZutSoPlMnyzXpFcKyHJc/edit#slide=id.p2][is here]].

The other great idea for this kind of long course was the idea of having a Tea
room and a Teachers lounge where people can just tune in to chat.
*** Caveats
A couple of things which keep cropping up for online teaching in general are the
following:
- Zoom does not have persistent chats, so an auxiliary tool like an [[https://board.net][Etherpad]] is great
* DONE Small Section On-boarding :@notes:teaching:cs106a:
CLOSED: [2020-04-14 Tue 02:48]
:PROPERTIES:
:EXPORT_FILE_NAME: scp-smallgrp
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:END:
** Background
As I mentioned in my [[https://rgoswami.me/posts/scp-onboarding/][last post]], I'm leading a section for [[https://compedu.stanford.edu/codeinplace/announcement/][Stanford CS106A: Code
in Place]]. I did also mention I'd try to keep a set of short notes on the
process. So there[fn:videos].
** The Training
Given the overwhelming number of students, and section leaders, the small groups
are for fostering a community of teachers.

# Arun Kulshrestha is the section leader. He graduated a while ago from Stanford
# and was a section leader too.

- [ ] Consider allowing for daisy chaining during introductions
- [ ] Discussions are the primary take-away
- [ ] Only the instructor should be coding during the session

*** Core components
- Clarity
- Content
- Atmosphere
- Section management
- Correctness
*** Sectional Details
- Check in at the start
- Notice the space
- Check in regularly
- Avoid negative phrases
- Establish norms and the general culture
*** Zoom Norms
- Have people introduce themselves
- Mute people when they aren't talking
- Raise hands
- Try to use icebreakers which respect privacy
*** Materials
Here's some of the stuff which, being as it was open-sourced, I suppose is OK to
put here[fn:help].
- [[https://docs.google.com/document/d/1PPei3a5yORmKW1KusD4kearBUzZZM8DYCG7X0NY1oaM/preview][Section Leader Training]]
- [[https://docs.google.com/document/d/1VTnPA7dMwqpoE_Dl-jWL32g99P_ey4g-NmF7OEzhqR8/preview][Section Leaders' Guide to Virtual Sections]]
- [[https://docs.google.com/document/d/1lHdnwAB17iLyvASZbWrIZz4PVy9zMmHjxGBGPwXNDs4/preview#heading=h.7dq0u3orjv9z][Some Zoom Icebreakers]]
[fn:help] If you know otherwise, let me know in the comments
[fn:videos] As you may know, the official playlist [[https://www.youtube.com/channel/UCWw34Ie0yNe96myEZ5RLHhg][is here]]
* DONE CS106A Section Meeting I :@notes:teaching:cs106a:
CLOSED: [2020-04-17 Fri 22:54]
:PROPERTIES:
:EXPORT_FILE_NAME: scp-smallgrp-meet1
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:END:
** Background
As I mentioned [[https://rgoswami.me/posts/scp-onboarding/][earlier]], I'm leading a section for [[https://compedu.stanford.edu/codeinplace/announcement/][Stanford CS106A: Code
in Place]]. I did also mention I'd try to keep a set of short notes on the
process. I finally had my first section meeting!
** Preparation
I went through the following:
- Sent out a welcome message
- Detailed the workflow
- Set up a HackMD instance
- Set up some slides in ~beamer~[fn:whyslides]

However, after that, I was still concerned since I didn't get much of a response on the ice-breakers for EdStem. Thankfully, everyone showed up.
** Teaching
- I had a fabulous session, and we went through a variety of concepts.
- Didn't spend much time on icebreakers, but did get a feel for where the students stand on the functional vs imperative programming paradigms
- Possibly because of working through two different approaches, the 40 minute long session went on for two hours and fifteen minutes.
- Some students had more of a background than the others, thankfully computational thinking is not normally taught very well

** Conclusion

- The notes are [[https://hackmd.io/tqd-a5SlSbK_NAtnSdqEPA][visible here]], and the session was [[https://youtu.be/rLak1v4k4o0][recorded here]][fn:whatnow]
- It was fun, and I hope the students enjoyed it as much as I did.
- I will probably expand this in terms of the concepts covered, to give the students more of an overview of what was covered

[fn:whyslides] Even though most of the session was supposed to be live, it was still helpful to show I was interested enough to set up slides
[fn:whatnow] As always, advice is much appreciated (and moderated)
* DONE CS106A Small Group Training :@notes:teaching:cs106a:
CLOSED: [2020-04-22 Wed 07:01]
:PROPERTIES:
:EXPORT_FILE_NAME: scp-smallgrp-trainig
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments false
:END:
** Background
As I mentioned [[https://rgoswami.me/posts/scp-onboarding/][earlier]], I'm leading a section for [[https://compedu.stanford.edu/codeinplace/announcement/][Stanford CS106A: Code
in Place]]. This post relates to the notes and thoughts garnered during the small group training session[fn:whenpost].
** Reflections
*** Demographics
Redacted. Did not use breakout meetings due to privacy issues.
*** Engagement and Participation
- Some people were more active (skewed responses)
- Some of the more rudimentary questions might have been suppressed
*** Highlighted Moments
- Covering multiple perspectives
- Different mental models
*** Challenges and Transformations
- Technical debt was an issue
- Lack of engagement
- Went on for too long
For me in particular:
#+BEGIN_QUOTE
It took over two hours, and though most people stayed on, not everyone was engaged.
#+END_QUOTE
** Scenarios
These are to be dealt with as per the [[https://docs.google.com/document/d/13RZzvY_9WTR_sjo_Y4oBNchsAWAv_z6kSJ9395snANU/preview][guidelines here]]. Since different groups covered different scenarios, not all of these have answers here.
*** Ensuring Engagement
#+BEGIN_QUOTE
You have some students who didn't participate at all in the section. What do you do?
#+END_QUOTE
*** Effective Communication
#+BEGIN_QUOTE
What might not be effective about the policy, ‚ÄúStudents should just tell me if I say something that offends them‚Äù?
#+END_QUOTE
*** Sharing Experiences
#+BEGIN_QUOTE
You just finished your section and are staying behind to answer questions from your students. A couple students asked what it‚Äôs like studying/working in an engineering/tech field.

What things might you want to keep in mind when answering their questions?
#+END_QUOTE
*** Time Management
#+BEGIN_QUOTE
Section went way over time due to lots of questions being asked by students. What are some time management strategies you can use moving forward?
#+END_QUOTE
*** Homework Assists
#+BEGIN_QUOTE
A sectionee posts in your Ed group, ‚ÄúI am a little bit frustrated because I don't really know where to start on the first assignment. A little hint would be very helpful.‚Äù How do you respond?
#+END_QUOTE
*** Debugging
#+BEGIN_QUOTE
A  sectionee shows you the following buggy code for printing all the elements in a list:

my_lst = ['apple', 'banana', 'carrot']
i = 0
while len(my_lst) > 0:
  print(my_lst[i])
  i = i + 1

They explain that the code works (it prints all the elements in the right order) but then throws a weird error: ‚ÄúIndexError: list index out of range.‚Äù  How would you help them find their bug?
#+END_QUOTE
*** Quitting
#+BEGIN_QUOTE
You have a student who is already discouraged by how difficult the first assignment is and has told you they don‚Äôt feel cut out for CS.  What do you say to them?
#+END_QUOTE
1. Provide encouragement
2. Give examples of hardship faced
3. Be positive and make sure they don‚Äôt feel worse, even if they do follow through and quit
4. ‚ÄúYou‚Äôre not the first‚Äù
5. Takes a lot of time. Doesn‚Äôt happen overnight
6. Ask them why they don‚Äôt feel cut out and try to solve that problem

*** Looking up issues
#+BEGIN_QUOTE
Why might it be problematic to say something like, ‚ÄúIt‚Äôs easy to download X or look up the answer to Y‚Äù? Why might those statements not be true?
#+END_QUOTE
1. Difficulty in backgrounds (language barriers)
2. They might not be able to understand stackoverflow.com until they learn more CS
3. They might not know where to look online (lack of domain expertise)
4. Dependencies (for downloads)
5. Makes them feel bad if they don‚Äôt end up finding it easy

[fn:whenpost] This post was created on the day of training, 21-04-20, but will be posted later
* DONE Using Mathematica with Orgmode :@programming:tools:emacs:workflow:orgmode:
CLOSED: [2020-04-26 Sun 20:01]
:PROPERTIES:
:EXPORT_FILE_NAME: org-mathematica
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
** Background
I have been wanting to find a workflow which allows me to bypass writing a lot of TeX by hand for a while now. To that end I looked into using a computer algebra system (CAS). Naturally, my first choice was the [[http://maxima.sourceforge.net/][FOSS Maxima]] (also because it uses Lisp under the hood). However, for all the reasons [[http://thingwy.blogspot.com/2015/07/maxima-versus-mathematica-should-i-go.html][listed here]], relating to its accuracy, which have not been fixed even though the post was over 5 years ago, I ended up having to go with the closed source [[https://www.wolfram.com/mathematica/][Mathematica]].
** Packages
Support for Mathematica in modern orgmode is mainly through the use of [[https://github.com/emacsmirror/org/blob/master/contrib/lisp/ob-mathematica.el][ob-mathematica]], which is the official org-babel extension (from ~contrib~) for working with Mathematica. However, ~ob-mathematica~ relies on the now-defunct ~mma~ package for font-locking, which is less than ideal. Thankfully, there exists the excellent [[https://github.com/kawabata/wolfram-mode][wolfram-mode]] package which happens to be in MELPA as well. Finally, since the default return type of a ~mathematica~ block is an input-string meant to be used in another ~mathematica~ block, which is not useful when we work with ~org-babel~, we will use the excellent ~mash.pl~ utility [[http://ai.eecs.umich.edu/people/dreeves/mash/][from here]], as suggested by the ~ob-mathematica~ package to sanitize our output and set a unifying path.

So to recap, use your favorite manager to get:
- [x] ~ob-mathematica~ (in contrib)
- [x] ~wolfram-mode~ ([[https://melpa.org/#/wolfram-mode][MELPA]])
- [x] ~mash.pl~ ([[http://ai.eecs.umich.edu/people/dreeves/mash][from here]])[fn:aboutmash]

After obtaining the packages, the configuration is then simply[fn:fullconf]:

#+begin_src emacs-lisp
;; Load mathematica from contrib
(org-babel-do-load-languages 'org-babel-load-languages
                             (append org-babel-load-languages
                                     '((mathematica . t))
                                     ))
;; Sanitize output and deal with paths
(setq org-babel-mathematica-command "~/.local/bin/mash")
;; Font-locking
(add-to-list 'org-src-lang-modes '("mathematica" . wolfram))
;; For wolfram-mode
(setq mathematica-command-line "~/.local/bin/mash")
#+end_src

** Results
*** LaTeX
Now we are in a position to simply evaluate content with font-locking. We will test our set up with an example lifted from the ~ob-mathematica~ [[https://github.com/analyticd/wy-els/blob/master/ob-mathematica.el][source-code]].

#+NAME: example-table
#+caption: A table
  | 1 | 4 |
  | 2 | 4 |
  | 3 | 6 |
  | 4 | 8 |
  | 7 | 0 |

#+BEGIN_SRC mathematica :var x=example-table :results latex
(1+Transpose@x) // TeXForm
#+END_SRC

#+RESULTS:
#+begin_export latex
\left(
\begin{array}{ccccc}
 2 & 3 & 4 & 5 & 8 \\
 5 & 5 & 7 & 9 & 1 \\
\end{array}
\right)
#+end_export

Where our header-line (with ~#+begin_src~) is:
#+BEGIN_SRC orgmode
mathematica :var x=example-table :results latex
#+END_SRC

*** Sanity Checks

We can also test the example from the [[http://thingwy.blogspot.com/2015/07/maxima-versus-mathematica-should-i-go.html][blog post]] earlier to test basic mathematical sanity.

#+BEGIN_SRC mathematica :results raw
Limit[Log[b - a + I eta], eta -> 0, Direction -> -1,Assumptions -> {a > 0, b > 0, a > b}]
TeXForm[Limit[Log[b - a + I eta], eta -> 0, Direction -> 1,Assumptions -> {a > 0, b > 0, a > b}]]
#+END_SRC

#+RESULTS:
$(I*Pi + Log[a - b])*\log (a-b)-i \pi$

*** Inline Math

Note that we can now also write fractions, integrals and other cumbersome TeX objects a lot faster with this syntax, like src_mathematica[:exports none :results raw]{Integrate[x^2,x] // TeXForm} $\frac{x^3}{3}$. Where we are using the following snippet:

#+BEGIN_SRC orgmode
src_mathematica[:exports none :results raw]{Integrate[x^2,x] // TeXForm}
#+END_SRC

*** Plots

For plots, the standard ~orgmode~ rules apply, that is, we have to export to a file and return the name through our code snippet. Consider:
#+BEGIN_SRC mathematica :results file
p=Plot[Sin[x], {x, 0, 6 Pi},Frame->True];
Export["images/sine.png",p];
Print["images/sine.png"]
#+END_SRC

#+RESULTS:
#+caption: An exported Mathematica image
[[file:images/sine.png]]

Where we have used ~mathematica :results file~ as our header line.

[fn:aboutmash] As noted in the comments, it is nicer to rename ~mash.pl~ to ~mash~
[fn:whydoom] The dark side has cookies
[fn:fullconf] For reference, my whole config [[https://dotdoom.rgoswami.me][is here]]
* TODO Thoughts on Chemical Engineering :@notes:ramblings:explanations:
:PROPERTIES:
:EXPORT_FILE_NAME: cheme-thoughts
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments false
:END:
** Background
** Mass and Energy Balances
From the first lecture on the topic, I came away with an implicit understanding, that though reactor design might be the heart of a chemical engineering plant, the soul would be mass and energy balances.
* DONE An Orgmode Note Workflow :@notes:ramblings:explanations:
CLOSED: [2020-05-10 Sun 15:01]
:PROPERTIES:
:EXPORT_FILE_NAME: org-note-workflow
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
** Background
One of the main reasons to use ~orgmode~ is definitely to get a better note
taking workflow. Closely related to blogging or writing, the ideal note workflow
is one which lets you keep a bunch of throwaway ideas and also somehow have
access to them in a coherent manner. This will be a long post, and it is a
work-in-progress, so, keep that in mind. Since this is mainly me[fn:bojackme]
work-shopping my technique, the philosophy will come in a later post probably.
This workflow is documented more sparsely in my [[https://dotdoom.rgoswami.me/config.html#text-3][config file here]], in the
~noteYoda~ section[fn:whoyoda]. Some parts of this post also include mini video
clips for clarity[fn:howrec].

The entire workflow will end up being something like this[fn:notrepro]:

{{< youtube UWB6ZABRVq0 >}}

** Concept
While working through ideas, it actually was more useful to describe the
workflow I want, and then implement it, instead of relying on the canned
approaches of each package. So the basics of the ideology are listed below.
*** Reference Management
Reference management is one of the main reasons to consider a plain-text setup, and mine is no different. The options most commonly seen are:
- Mendeley :: This is a great option, and the most mobile friendly of the bunch. Sadly, the price tiers aren't very friendly so I have to give it a hard pass.
- Jabref :: This is fun, but really more of a per-project management system, but it works well for that. The fact that it is Java based was a major issue for me.
- Zotero :: This is what I personally use and recommend. More on that in a later post.
*** Notes
The idea is to be able to create notes for all kinds of content. Specifically,
papers or books, along with webpages. This then requires a separate system for
each which is described by:
- Search Engine :: The search engine is key, both in terms of accessibility and scalability. It is assumed that there will be many notes, and that they will have a wide variety of content. The search interface must then simply allow us to narrow down our candidates in a meaningful manner.
- Contextual Representation :: This aspect of the workflow deals with representations, which should transcend the usage of tags or categories. In particular, it would be nice to be able to visualize the flow of ideas, each represented by a note.
- Backlinks :: In particular, by backlinks at this point we are referring to the ability to link to a ~pdf~ or a website with a unique key such that notes can be added or removed at will.
- Storage :: Not actually part of the workflow in the same way, since it will be handled at the system level, it is worth nothing, that in this workflow Zotero is used to export a master *bib* file and keeps it updated, while the notes themselves are version controlled[fn:nodrop].
The concepts above will be handled by the following packages.

| Concept   | Package                      | Note                                                   |
|-----------+------------------------------+--------------------------------------------------------|
| Search    | deft                         | Has a great interface                                  |
| Context   | org-roam                     | Allows the export of graphiz mindmaps                  |
| Backlinks | org-roam, org-ref, org-noter | Covers websites, bibliographies, and pdfs respectively |

A key component in this workflow is actually facilitated by the fabulous
~org-roam-bibtex~ [[https://github.com/org-roam/org-roam-bibtex][or ORB]]. The basic idea is to ensure meaningful templates which
interpolate smoothly with ~org-roam~, ~org-ref~, ~helm-bibtex~, and
~org-capture~.
*** Basic Variables
Given the packages we will be using, some variable settings are in order, namely:
#+BEGIN_SRC emacs-lisp
(setq
   org_notes (concat (getenv "HOME") "/Git/Gitlab/Mine/Notes/")
   zot_bib (concat (getenv "HOME") "/GDrive/zotLib.bib")
   org-directory org_notes
   deft-directory org_notes
   org-roam-directory org_notes
   )
#+END_SRC
** Search
For the search setup, the ~doom-emacs~ ~deft~ setup, by adding ~+deft~ in my
~init.el~, worked out of the box for me. For those who do not use
~doom~[fn:whynot], the following should suffice:
#+BEGIN_SRC emacs-lisp
(use-package deft
  :commands deft
  :init
  (setq deft-default-extension "org"
        ;; de-couples filename and note title:
        deft-use-filename-as-title nil
        deft-use-filter-string-for-filename t
        ;; disable auto-save
        deft-auto-save-interval -1.0
        ;; converts the filter string into a readable file-name using kebab-case:
        deft-file-naming-rules
        '((noslash . "-")
          (nospace . "-")
          (case-fn . downcase)))
  :config
  (add-to-list 'deft-extensions "tex")
  )
#+END_SRC
For more about the ~doom-emacs~ defaults, check [[https://github.com/hlissner/doom-emacs/search?q=deft&unscoped_q=deft][the Github repo]]. The other
aspect of interacting with the notes is via the ~org-roam~ interface and will be
covered below.
** Bibliography
Since I will be using ~org-ref~, it makes no sense to load or work with the
~+biblio~ module at the moment. Thus this section is actually ~doom~ agnostic.
The basic tools of bibliographic management from the ~emacs~ end are the
venerable ~helm-bibtex~ ([[https://github.com/tmalsburg/helm-bibtex][repo here]]) and ~org-ref~ ([[https://github.com/jkitchin/org-ref/][repo here]]). In order to make
this guide complete, I will also describe the [[https://www.zotero.org/][Zotero]] settings I have.
*** Zotero
Without getting too deep into the weeds here, the basic requirements are:
- [x] [[https://zotero.org][Zotero]]
- [x] The [[https://retorque.re/zotero-better-bibtex/][better bibtex extension]]
The idea is to then have one top level ~.bib~ file in some handy location which
you will set up to sync automatically. To make life easier, there is a tiny
recording of the next steps.

{{< youtube iDRpIo7mcKE >}}

** Helm-Bibtex
This [[https://github.com/tmalsburg/helm-bibtex][venerable package]] is really good at interfacing with a variety of
externally formatted bibliographic managers.
#+BEGIN_SRC emacs-lisp
(setq
 bibtex-completion-notes-path "/home/haozeke/Git/Gitlab/Mine/Notes/"
 bibtex-completion-bibliography "/home/haozeke/GDrive/zotLib.bib"
 bibtex-completion-pdf-field "file"
 bibtex-completion-notes-template-multiple-files
 (concat
  "#+TITLE: ${title}\n"
  "#+ROAM_KEY: cite:${=key=}\n"
  "* TODO Notes\n"
  ":PROPERTIES:\n"
  ":Custom_ID: ${=key=}\n"
  ":NOTER_DOCUMENT: %(orb-process-file-field \"${=key=}\")\n"
  ":AUTHOR: ${author-abbrev}\n"
  ":JOURNAL: ${journaltitle}\n"
  ":DATE: ${date}\n"
  ":YEAR: ${year}\n"
  ":DOI: ${doi}\n"
  ":URL: ${url}\n"
  ":END:\n\n"
  )
 )
#+END_SRC
~doom-emacs~ users like me might want to wrap the above in a nice ~after!
org-ref~ expression, but it doesn't really matter.
*** Explanation
To break-down aspects of the configuration snippet above:
- The template includes the ~orb-process-file-field~ function to allow selecting the ~pdf~ to be used with ~org-noter~
- The ~file~ field is specified to work with the ~.bib~ file generated by Zotero
- ~helm-bibtex~ allows for any of the keys in a ~.bib~ file to be used in a template, and an overly expressive one is more useful
- The ~ROAM_KEY~ is defined to ensure that cite backlinks work correctly with ~org-roam~
- As I prefer to have one notes file per ~pdf~, I have only configured the ~bibtex-completion-notes-template-multiple-files~ variable
** Org-Ref
As discussed above, this just makes citations much more meaningful in ~orgmode~.
#+BEGIN_SRC emacs-lisp
(use-package org-ref
    :config
    (setq
         org-ref-completion-library 'org-ref-ivy-cite
         org-ref-get-pdf-filename-function 'org-ref-get-pdf-filename-helm-bibtex
         org-ref-default-bibliography (list "/home/haozeke/GDrive/zotLib.bib")
         org-ref-bibliography-notes "/home/haozeke/Git/Gitlab/Mine/Notes/bibnotes.org"
         org-ref-note-title-format "* TODO %y - %t\n :PROPERTIES:\n  :Custom_ID: %k\n  :NOTER_DOCUMENT: %F\n :ROAM_KEY: cite:%k\n  :AUTHOR: %9a\n  :JOURNAL: %j\n  :YEAR: %y\n  :VOLUME: %v\n  :PAGES: %p\n  :DOI: %D\n  :URL: %U\n :END:\n\n"
         org-ref-notes-directory "/home/haozeke/Git/Gitlab/Mine/Notes/"
         org-ref-notes-function 'orb-edit-notes
    ))
#+END_SRC
An essential aspect of this configuration is just that most of heavy lifting in
terms of the notes are palmed off to ~helm-bibtex~.
*** Explanation
To break-down aspects of the configuration snippet above:
- The ~org-ref-get-pdf-filename-function~ simply uses the ~helm-bibtex~ settings to find the ~pdf~
- The default bibliography and notes directory are set to the same location as all the ~org-roam~ files, to encourage a flat hierarchy
- The ~org-ref-notes-function~ simply ensures that, like the ~helm-bibtex~ settings, I expect one file per ~pdf~, and that I would like to use my ~org-roam~ template instead of the ~org-ref~ or ~helm-bibtex~ one
Note that for some reason,
the format specifiers for ~org-ref~ are *not* the keys in ~.bib~ but are
instead, the following[fn:wherereforg]:
#+BEGIN_SRC bash
In the format, the following percent escapes will be expanded.
%l   The BibTeX label of the citation.
%a   List of author names, see also `reftex-cite-punctuation'.
%2a  Like %a, but abbreviate more than 2 authors like Jones et al.
%A   First author name only.
%e   Works like %a, but on list of editor names.  (%2e and %E work as well)
It is also possible to access all other BibTeX database fields:
%b booktitle     %c chapter        %d edition    %h howpublished
%i institution   %j journal        %k key        %m month
%n number        %o organization   %p pages      %P first page
%r address       %s school         %u publisher  %t title
%v volume        %y year
%B booktitle, abbreviated          %T title, abbreviated
%U url
%D doi
%S series        %N note
%f pdf filename
%F absolute pdf filename
Usually, only %l is needed.  The other stuff is mainly for the echo area
display, and for (setq reftex-comment-citations t).
%< as a special operator kills punctuation and space around it after the
string has been formatted.
A pair of square brackets indicates an optional argument, and RefTeX
will prompt for the values of these arguments.
#+END_SRC
** Indexing Notes
This part of the workflow builds on the concepts best known as the [[https://www.zettelkasten.de/][Zettelkasten method]]. More details about the philosophy behind ~org-roam~ is [[https://org-roam.readthedocs.io/en/latest/][here]].
*** Org-Roam
The first part of this interface is essentially just the ~doom-emacs~
configuration, adapted for those who don't believe in the dark side below.
#+BEGIN_SRC emacs-lisp
(use-package org-roam
  :hook (org-load . org-roam-mode)
  :commands (org-roam-buffer-toggle-display
             org-roam-find-file
             org-roam-graph
             org-roam-insert
             org-roam-switch-to-buffer
             org-roam-dailies-date
             org-roam-dailies-today
             org-roam-dailies-tomorrow
             org-roam-dailies-yesterday)
  :preface
  ;; Set this to nil so we can later detect whether the user has set a custom
  ;; directory for it, and default to `org-directory' if they haven't.
  (defvar org-roam-directory nil)
  :init
  :config
  (setq org-roam-directory (expand-file-name (or org-roam-directory "roam")
                                             org-directory)
        org-roam-verbose nil  ; https://youtu.be/fn4jIlFwuLU
        org-roam-buffer-no-delete-other-windows t ; make org-roam buffer sticky
        org-roam-completion-system 'default

  ;; Normally, the org-roam buffer doesn't open until you explicitly call
  ;; `org-roam'. If `+org-roam-open-buffer-on-find-file' is non-nil, the
  ;; org-roam buffer will be opened for you when you use `org-roam-find-file'
  ;; (but not `find-file', to limit the scope of this behavior).
  (add-hook 'find-file-hook
    (defun +org-roam-open-buffer-maybe-h ()
      (and +org-roam-open-buffer-on-find-file
           (memq 'org-roam-buffer--update-maybe post-command-hook)
           (not (window-parameter nil 'window-side)) ; don't proc for popups
           (not (eq 'visible (org-roam-buffer--visibility)))
           (with-current-buffer (window-buffer)
             (org-roam-buffer--get-create)))))

  ;; Hide the mode line in the org-roam buffer, since it serves no purpose. This
  ;; makes it easier to distinguish among other org buffers.
  (add-hook 'org-roam-buffer-prepare-hook #'hide-mode-line-mode))


;; Since the org module lazy loads org-protocol (waits until an org URL is
;; detected), we can safely chain `org-roam-protocol' to it.
(use-package org-roam-protocol
  :after org-protocol)


(use-package company-org-roam
  :after org-roam
  :config
  (set-company-backend! 'org-mode '(company-org-roam company-yasnippet company-dabbrev)))
#+END_SRC
Once again, for more details, check the [[https://github.com/hlissner/doom-emacs/search?q=roam&unscoped_q=roam][Github repo]].
*** Org-Roam-Bibtex
The configuration required is:
#+BEGIN_SRC emacs-lisp
 (use-package org-roam-bibtex
  :after (org-roam)
  :hook (org-roam-mode . org-roam-bibtex-mode)
  :config
  (setq org-roam-bibtex-preformat-keywords
   '("=key=" "title" "url" "file" "author-or-editor" "keywords"))
  (setq orb-templates
        '(("r" "ref" plain (function org-roam-capture--get-point)
           ""
           :file-name "${slug}"
           :head "#+TITLE: ${=key=}: ${title}\n#+ROAM_KEY: ${ref}

- tags ::
- keywords :: ${keywords}

\n* ${title}\n  :PROPERTIES:\n  :Custom_ID: ${=key=}\n  :URL: ${url}\n  :AUTHOR: ${author-or-editor}\n  :NOTER_DOCUMENT: %(orb-process-file-field \"${=key=}\")\n  :NOTER_PAGE: \n  :END:\n\n"

           :unnarrowed t))))
#+END_SRC
Where most of the configuration is essentially the template again. Like ~helm-bibtex~, [[https://github.com/org-roam/org-roam-bibtex][ORB]] allows taking arbitrary keys from the ~.bib~ file.
** Org Noter
The final aspect of a ~pdf~ workflow is simply ensuring that every ~pdf~ is
associated with notes. The philosophy of ~org-noter~ is [[https://github.com/weirdNox/org-noter][best described here]].
Only minor tweaks should be required to get this working with ~interleave~ as
well.
#+BEGIN_SRC emacs-lisp
(use-package org-noter
  :after (:any org pdf-view)
  :config
  (setq
   ;; The WM can handle splits
   org-noter-notes-window-location 'other-frame
   ;; Please stop opening frames
   org-noter-always-create-frame nil
   ;; I want to see the whole file
   org-noter-hide-other nil
   ;; Everything is relative to the main notes file
   org-noter-notes-search-path (list org_notes)
   )
  )
#+END_SRC
Evidently, from my configuration, it appears that I decided to use [[https://github.com/weirdNox/org-noter][org-noter]] over
the more commonly described [[https://github.com/rudolfochrist/interleave][interleave]] because it has better support for working
with multiple documents linked to one file.
** Org-Protocol
I will only cover the bare minimum relating to the use of ~org-capture~ here,
because eventually I intend to handle a lot more cases with [[https://github.com/abo-abo/orca][orca]]. Note that this
part of the workflow has more to do with using ~org-roam~ with websites than
~pdf~ files.
*** Templates
This might get complicated but I am only trying to get the bare minimum for
~org-protocol~ right now.
#+BEGIN_SRC emacs-lisp
;; Actually start using templates
(after! org-capture
  ;; Firefox and Chrome
  (add-to-list 'org-capture-templates
               '("P" "Protocol" entry ; key, name, type
                 (file+headline +org-capture-notes-file "Inbox") ; target
                 "* %^{Title}\nSource: %u, %c\n #+BEGIN_QUOTE\n%i\n#+END_QUOTE\n\n\n%?"
                 :prepend t ; properties
                 :kill-buffer t))
  (add-to-list 'org-capture-templates
               '("L" "Protocol Link" entry
                 (file+headline +org-capture-notes-file "Inbox")
                 "* %? [[%:link][%(transform-square-brackets-to-round-ones \"%:description\")]]\n"
                 :prepend t
                 :kill-buffer t))
)
#+END_SRC
** Conclusions
At this point, many might argue that since by the end, only one template is
called, defining the rest were pointless. They would be right, however, this is
just how my configuration evolved. Feel free to cannibalize this for your
personal benefit. Eventually I plan to expand this into something with
~org-journal~ as well, but not right now.
[fn:notrepro] The video uses ~org-ref-notes-function-many-files~ as the ~org-ref-notes-function~ so the template looks a little different
[fn:nodrop] For some strange reason a lot of online posts suggested Dropbox for syncing notes, which makes no sense to me, it is always better to have version control and ignore rules
[fn:wherereforg] Where these are from the [[https://github.com/jkitchin/org-ref/blob/875371a63430544e446db7a76b44b33c7e20a8bd/org-ref-utils.el][org-ref documentation]]
[fn:howrec] Recorded with [[https://www.maartenbaert.be/simplescreenrecorder/][SimpleScreenRecorder]], cut with [[https://github.com/mifi/lossless-cut][LosslessCut]], uploaded to [[https://www.youtube.com/][YouTube]], and embedded with a [[https://gohugo.io/extras/shortcodes/#youtube][Hugo shortcode]]
[fn:bojackme] Rohit Goswami that is, from the [[https://rgoswami.me][landing page]]; obviously
[fn:whoyoda] This is a reference to my fantastic pet, named Yoda
[fn:whynot] Therefore clearly proving that the cookies of the dark side have no power in the holy text editor war
* DONE Pandoc to Orgmode with Babel :@programming:tools:emacs:workflow:orgmode:
CLOSED: [2020-05-02 Sat 16:39]
:PROPERTIES:
:EXPORT_FILE_NAME: org-pandoc-babel
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
** Background
One of the best things about writing in ~orgmode~ is that we can embed and
execute arbitrary code snippets. However, not all languages have an exporter,
for obvious reasons. Somewhat surprisingly, there is no way to call [[https://pandoc.org/MANUAL.html][pandoc]] on embedded snippets, which feels like a waste, especially when a whole bunch of documentation formats can be converted to ~orgmode~ with it.

Consider the following beautifully highlighted snippet of an ~rst~ (ReStructured Text) [[https://sublime-and-sphinx-guide.readthedocs.io/en/latest/tables.html][list table]].
#+BEGIN_SRC rest
.. list-table:: Title
   :widths: 25 25 50
   :header-rows: 1

   ,* - Heading row 1, column 1
     - Heading row 1, column 2
     - Heading row 1, column 3
   ,* - Row 1, column 1
     -
     - Row 1, column 3
   ,* - Row 2, column 1
     - Row 2, column 2
   - Row 2, column 3
#+END_SRC

Trying to run this will generate the sort of obvious error:
#+BEGIN_SRC emacs-lisp
org-babel-execute-src-block: No org-babel-execute function for rst!
#+END_SRC
** Writing an Exporter
For this post, I will be focusing on ~rst~, but this can be defined for any of the ~pandoc~ back-ends. The approach was inspired by [[https://github.com/hoelterhof/ob-markdown/blob/master/ob-markdown.el][ob-markdown]].

#+BEGIN_SRC emacs-lisp
(defun org-babel-execute:rst (body params)
  "Execute a block of rst code with org-babel.
This function is called by `org-babel-execute-src-block'."
  (let* ((result-params (split-string (or (cdr (assoc :results params)) "")))
	 (in-file (org-babel-temp-file "rst-"))
	 (cmdline (cdr (assoc :cmdline params)))
	 (to (cdr (assoc :to params)))
	 (template (cdr (assoc :template params)))
	 (cmd (concat "pandoc"
		      " -t  org"
		      " -i " (org-babel-process-file-name in-file)
		      " -f rst "
		      " " cmdline)))
    (with-temp-file in-file (insert body))
    (message cmd)
    (shell-command-to-string cmd))) ;; Send to results

(defun org-babel-prep-session:rst (session params)
  "Return an error because rst does not support sessions."
  (error "rst does not support sessions"))
#+END_SRC

** Trying it out
With that done, it is pretty trivial to re-run the above example.

#+BEGIN_SRC rest :exports both :results raw
.. list-table:: Title
   :widths: 25 25 50
   :header-rows: 1

   ,* - Heading row 1, column 1
     - Heading row 1, column 2
     - Heading row 1, column 3
   ,* - Row 1, column 1
     -
     - Row 1, column 3
   ,* - Row 2, column 1
     - Row 2, column 2
   - Row 2, column 3
#+END_SRC

#+RESULTS:
| Heading row 1, column 1 | Heading row 1, column 2 | Heading row 1, column 3 |
|-------------------------+-------------------------+-------------------------|
| Row 1, column 1         |                         | Row 1, column 3         |
| Row 2, column 1         | Row 2, column 2         |                         |
|                         |                         |                         |
#+CAPTION: Title

Note that we have used ~rst :exports both :results raw~ as the header argument.

** Conclusions
Will probably follow this up with an actual package, which should handle the entire spectrum of ~pandoc~ back-ends.
* DONE Refactoring Dotfiles For Colemak :@programming:workflow:
CLOSED: [2020-05-02 Sat 20:30]
:PROPERTIES:
:EXPORT_FILE_NAME: colemak-dots-refactor
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
** Background
I have, in the past written about how I [[Switching to Colemak][made the switch]] to Colemak. However,
until recently, I was still trying to mimic the VIM keybindings from QWERTY.
This is a post where I discuss the changes I made to ensure that I never have to
stretch my fingers in odd ways again. The main idea is expressed well by [[https://github.com/jooize/vim-colemak][vim-colemak]].
#+BEGIN_SRC shell
Colemak layout:                  |                 QWERTY layout:
`12345 67890-=     Move around:  |  (instead of)   `12345 67890-=
 qwfpg jluy;[]\         e        |       k          qwert yuiop[]\
 arstd HNEIo'         h   i      |     h   l        asdfg HJKL;'
 zxcvb km,./            n        |       j          zxcvb nm,./
#+END_SRC
** Emacs
Though I have [[https://github.com/hlissner/doom-emacs/issues/783#issuecomment-535805347][mentioned publicly]], that I was using the regular QWERTY motion
keys, I realized I had actually started to use the mouse more often, simply
because it was a pain to navigate. Thankfully, ~emacs~ has [[https://github.com/wbolster/evil-colemak-basics][evil-colemak-basics]],
which is fabulous. For reference, these make it really easy for QWERTY users to
make the switch if they're previously used to VIM bindings.
#+BEGIN_SRC rst :results raw :exports results
.. list-table::
   :header-rows: 1

   * - Colemak
     - Qwerty
     - Action
     - States
     - At Qwerty position?
     - Remarks

   * - ``h``, ``n``, ``e``, ``i``
     - ``h``, ``j``, ``k``, ``l``
     - navigate
     - ``mnvo``
     - yes
     -

   * - ``k``, ``K``
     - ``n``, ``N``
     - search next/previous
     - ``mnvo``
     - yes
     -

   * - ``u``, ``U``
     - ``i``, ``I``
     - insert
     - ``_nv_``
     - yes
     -

   * - ``l``
     - ``u``
     - undo
     - ``_nv_``
     - yes
     -

   * - ``N``
     - ``J``
     - join lines
     - ``_nv_``
     - yes
     -

   * - ``E``
     - ``K``
     - lookup
     - ``mnv_``
     - yes
     -

   * - ``u``
     - ``i``
     - inner text object keymap
     - ``___o``
     - yes
     -

   * - ``f``, ``F``
     - ``e``, ``E``
     - jump to end of word
     - ``mnvo``
     - yes
     - with ``t-f-j`` rotation

   * - ``t``, ``T``
     - ``f``, ``f``
     - jump to character
     - ``mnvo``
     - yes
     - with ``t-f-j`` rotation

   * - ``j``, ``J``
     - ``t``, ``T``
     - jump until character
     - ``mnvo``
     - no
     - with ``t-f-j`` rotation

   * - ``j``, ``J``
     - ``e``, ``E``
     - jump to end of word
     - ``mnvo``
     - no
     - without ``t-f-j`` rotation
#+END_SRC

#+RESULTS:
| Colemak            | Qwerty             | Action                   | States | At Qwerty position? | Remarks                  |
|--------------------+--------------------+--------------------------+--------+---------------------+--------------------------|
| =h=, =n=, =e=, =i= | =h=, =j=, =k=, =l= | navigate                 | =mnvo= | yes                 |                          |
| =k=, =K=           | =n=, =N=           | search next/previous     | =mnvo= | yes                 |                          |
| =u=, =U=           | =i=, =I=           | insert                   | =_nv_= | yes                 |                          |
| =l=                | =u=                | undo                     | =_nv_= | yes                 |                          |
| =N=                | =J=                | join lines               | =_nv_= | yes                 |                          |
| =E=                | =K=                | lookup                   | =mnv_= | yes                 |                          |
| =u=                | =i=                | inner text object keymap | =___o= | yes                 |                          |
| =f=, =F=           | =e=, =E=           | jump to end of word      | =mnvo= | yes                 | with =t-f-j= rotation    |
| =t=, =T=           | =f=, =f=           | jump to character        | =mnvo= | yes                 | with =t-f-j= rotation    |
| =j=, =J=           | =t=, =T=           | jump until character     | =mnvo= | no                  | with =t-f-j= rotation    |
| =j=, =J=           | =e=, =E=           | jump to end of word      | =mnvo= | no                  | without =t-f-j= rotation |

Where the table above is from the fantastic [[https://github.com/wbolster/evil-colemak-basics][readme]].

I still had some issues, mostly relating to searching in buffers, so I ended
using ~swiper-isearch~ more which is a bonus too.

*** Visual Lines
Since I tend to keep ~visual-line-mode~ all the time, it makes sense to actually swap working with lines and visual lines.
To work this through this needs [[https://github.com/YourFin/evil-better-visual-line/][evil-better-visual-line]].
#+BEGIN_SRC emacs-lisp
(use-package! evil-better-visual-line
  :after evil-colemak-basics
  :config
  (evil-better-visual-line-on)
  (map! :map evil-colemak-basics-keymap
        (:nvm "n" 'evil-better-visual-line-next-line
         :nvm "e" 'evil-better-visual-line-previous-line
         :nvm "g n" 'evil-next-line
         :nvm "g e" 'evil-previous-line))
)
#+END_SRC
*** Pdf-Tools
For my ~doom-emacs~ configuration, I also set the following map:
#+BEGIN_SRC emacs-lisp
(after! pdf-view
  (add-hook! 'pdf-view-mode-hook (evil-colemak-basics-mode -1))
 (map!
   :map pdf-view-mode-map
   :n "g g"          #'pdf-view-first-page
   :n "G"            #'pdf-view-last-page
   :n "N"            #'pdf-view-next-page-command
   :n "E"            #'pdf-view-previous-page-command
   :n "e"            #'evil-collection-pdf-view-previous-line-or-previous-page
   :n "n"            #'evil-collection-pdf-view-next-line-or-next-page
 )
#+END_SRC
Where the most important thing is the hook which removes the
~evil-colemak-basics~ binding. Since it is a single mode and hook, ~after-hook!~
is the same as ~after-hook~[fn:attribute].
*** Window Management
Somehow these are not part of the ~evil-colemak~ defaults.
#+BEGIN_SRC emacs-lisp
(after! evil
  (map! :map evil-window-map
        (:leader
         (:prefix ("w" . "Select Window")
          :n :desc "Left"  "h" 'evil-window-left
          :n :desc "Up"    "e" 'evil-window-up
          :n :desc "Down"  "n" 'evil-window-down
          :n :desc "Right" "i" 'evil-window-right
          ))
        ))
#+END_SRC
*** Search
Harmonizing with Vimium.
#+BEGIN_SRC emacs-lisp
(after! evil (map! :map evil-motion-state-map
                   (:n :desc "Previous match" "K" 'evil-ex-search-previous
                    :n :desc "Next match" "k" 'evil-ex-search-next
                    :n :desc "Forward search" "/" 'evil-search-forward
                    )
                   ))
#+END_SRC
*** Page Movement
Though this is more of a personal preference, I find it more natural to bind N
and E to page-wise movement instead of join lines and lookup, since I almost
never use those commands, and the movement keys echo what I expect elsewhere.
#+BEGIN_SRC emacs-lisp
(after! evil
  (map! :map evil-colemak-basics-keymap
      :nv "N" 'evil-scroll-page-up
      :nv "E" 'evil-scroll-page-down)
  )
#+END_SRC
*** Evil Org
Annoyingly, ~evil-org-mode~ had a map which kept overriding all my other
settings. Thankfully it has a helper variable to set movement. I also do not
need this anyway, at-least not by default.
#+BEGIN_SRC emacs-lisp
(after! org
  (remove-hook 'org-mode-hook 'evil-org-mode)
  (setq evil-org-movement-bindings
        '((up . "e") (down . "n")
          (left . "h") (right . "i"))
        )
)
#+END_SRC
** Vimium
I use the excellent [[https://vimium.github.io/][vimium]] to make Chrome be a little less annoying. Luckily [[https://github.com/philc/vimium/wiki/colemak][the
Wiki]] seems to have a reasonable suggestion for colemak. The basic idea is to
migrate the underlying keys directly to ensure very few manual changes are
required.
#+BEGIN_SRC conf
mapkey n j
mapkey N J
mapkey e k
mapkey E K
mapkey i l
mapkey I L
mapkey k n
mapkey K N
mapkey l i
mapkey L I
mapkey j e
mapkey J E
#+END_SRC
** Zsh
To ensure uniform bindings, I used to use ~bindkey -v~ but will need some minor
changes to that set up. I based this part of my configuration off the bindings
of [[https://github.com/bunnyfly/dotfiles/blob/master/zshrc][bunnyfly]].
#+BEGIN_SRC shell
bindkey -v
# Colemak.
  bindkey -M vicmd "h" backward-char
  bindkey -M vicmd "n" down-line-or-history
  bindkey -M vicmd "e" up-line-or-history
  bindkey -M vicmd "i" forward-char
  bindkey -M vicmd "s" vi-insert
  bindkey -M vicmd "S" vi-insert-bol
  bindkey -M vicmd "k" vi-repeat-search
  bindkey -M vicmd "K" vi-rev-repeat-search
  bindkey -M vicmd "l" beginning-of-line
  bindkey -M vicmd "L" end-of-line
  bindkey -M vicmd "j" vi-forward-word-end
  bindkey -M vicmd "J" vi-forward-blank-word-end

# Sane Undo, Redo, Backspace, Delete.
  bindkey -M vicmd "u" undo
  bindkey -M vicmd "U" redo
  bindkey -M vicmd "^?" backward-delete-char
  bindkey -M vicmd "^[[3~" delete-char

# Keep ctrl+r searching
  bindkey -M viins '^R' history-incremental-pattern-search-forward
  bindkey -M viins '^r' history-incremental-pattern-search-backward
#+END_SRC
** Zathura
There is no better ~pdf~ viewer than [[https://pwmt.org/projects/zathura/][zathura]], and it also works for ~djvu~ and
friends. As a plus point, it normally has very reasonable ~vim~ bindings, and an
excellent configuration system, so we will leverage that. The best part is that
we can just add to it using ~include zathuraColemak~ or whatever so as to be
minimally invasive.

#+BEGIN_SRC vim
map h scroll left
map n scroll down
map e scroll up
map i scroll right

map N scroll half-down
map E scroll half-up

map k search forward
map K search backward

# For TOC navigation
map [index] o toggle_index

# hjkl ‚Üí  hnei
map [index] n navigate_index down
map [index] e navigate_index up
map [index] h navigate_index collapse
map [index] i navigate_index expand

map [index] H navigate_index collapse-all
map [index] I navigate_index expand-all
#+END_SRC
Zathura is a complicated beast, however, and my [[https://github.com/HaoZeke/Dotfiles/blob/master/dotfiles/colemak/.zshColemak][full configuration]] contains a
lot more information.
** i3
I have some bindings set up in terms of $left $right $up and $down, so it was
simple to re-bind them.
#+BEGIN_SRC vim
set $left h
set $down n
set $up e
set $right i
#+END_SRC
** Conclusions
That seems to be it for now. If I think of more programs I use regularly which
allow VIM bindings, or keybindings in general, I'll probably just update this
post. My full dotfiles are [[https://github.com/HaoZeke/Dotfiles][present here]], and now include a ~colemak~ target.
[fn:attribute] The hook fix was suggested by the fantastic [[https://github.com/hlissner/][hlissner]] on the super friendly doom Discord server.
* TODO Reclaiming The Web With RSS :@notes:ramblings:workflow:tools:
:PROPERTIES:
:EXPORT_FILE_NAME: reclaim-web-rss
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
** Background
I came across some fascinating sites recently, which didn't bother to have an RSS feed. I've been an RSS fan for as long as I can remember, so, that's really about it.
** Context
*** Do I need an RSS reader?
If you follow a bunch of talented, but sporadic content creators, then *yes*. If you plan on subscribing to news-feeds and consolidated feeds which spam you with a million links per minute, then *no*. To clarify, the feed reader setup is essentially a mechanism by which the audience opts-in to post subscriptions, but without putting the onus on the creator to harvest user-emails or any other data[fn:privacy].
*** RSS? Atom? asrtinuo?
One of the words in the title is not a feed. That said, the concept of a feed, given the rise of Facebook, is probably not hard for anyone to understand anymore. The specifications are a little different, though in practice from an end-user perspective, there are few (if any) differences.
**** Halp?
There are excellent guidelines for every static site generator, and
** Programs
First of all, *no one* really needs to have an RSS subscription. This actually applies to *every* provider, from Feedly, Inoreader, The Old Reader, etc. to even FOSS projects like Miniflux[fn:supportfoss]. Locally, there are a couple of options, but I will only cover things I have used personally.
*** RSSOwl
RSSOwl was fantastic. It looked great, ran smoothly, and had nifty desktop integration. That is, until it stopped working. Though it is a Java package so it is always possible to downgrade your JRE and JDK, sadly this is probably not worth the effort.
*** News Flash
A successor to the well-known Feedbin, this seems to check most of the boxes for me. Though it does not come with a tray icon, KDocker is quick to step into that breach.
** Personal Workflow
While consuming content, there are only a couple of things to do.
** Conclusions
I am not sure there will ever be a resurgence of RSS feeds, given the rise of newsletters. It would be nice to see more people get back to RSS though.
- [x] Don't be the product
- [x] Don't be beholden to search engine rankings
- [x] Support content creators, not SEO specialists

[fn:supportfoss] That said, you should always try to support FOSS products, so if you really feel the need to have a central server, go FOSS
[fn:privacy] That last bit, is probably why companies like [[http://xanadu.ai/][Xanadu]] and others prefer to have a mailing list....

* DONE Compton to Picom and Zoom Glitches :@notes:workflow:tools:
CLOSED: [2020-05-12 Tue 01:32]
:PROPERTIES:
:EXPORT_FILE_NAME: compton-zoom-shadow
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
** Background
I [[https://rgoswami.me/tags/cs106a/][have been leading]] the fantastic section 881 as a virtual section leader for
the [[https://compedu.stanford.edu/codeinplace/v1/#/course][Stanford CS106A: Code in Place]] initiative for the past four weeks. I have
also spent a lot of time on Zoom, sharing my screen. Fun fact. My screen shares
look like this:

#+DOWNLOADED: screenshot @ 2020-05-12 01:05:38
#+caption: Zoom screen share with weird overlay
[[file:images/Background/2020-05-12_01-05-38_screenshot.png]]

This post is about hunting down what caused this amazing zoom
glitch[fn:whatglitch] and how I finally fixed it.
** Tiling Windows and Compositors
For reasons best left to another post, I use the fabulous [[https://i3wm.org/][i3 window manager]],
with colemak keybindings [[i3][described here]]. Recall that, [[https://en.wikipedia.org/wiki/Compositing_window_manager][from Wikipedia]]:
#+BEGIN_QUOTE
A compositing window manager is a window manager that provides applications with an off-screen buffer for each window. The window manager composites the window buffers into an image representing the screen and writes the result into the display memory.
#+END_QUOTE

For reasons I can no longer recall, ~compton~ has been a traditional aspect of my
workflow. As per my last update back in April last year; my configuration [[https://github.com/HaoZeke/Dotfiles/blob/8213473f313aa8fb9eb4c22d6b36a801b1584df6/dotfiles/archLinux/.config/compton.conf][is here]].
** Compton to Picom
Some time ago (actually [[https://github.com/yshui/picom/issues/222][many months ago]]), ~compton~
itself transitioned over to ~picom~, but remained largely compatible with my old
configuration[fn:archwikiplug].
To be clear, the transition was largely painless, with ample warnings in the
terminal showing up; along with very reasonable fallbacks. The key aspect of my ~compton.conf~ which caused the shadowing was:
#+BEGIN_SRC shell
shadow = true;
shadow-radius = 5;
shadow-offset-x = -5;
shadow-offset-y = -5;
shadow-opacity = 0.5;
#+END_SRC
The corrective measure was simply to set ~shadow-opacity~ to nothing; that is:
#+BEGIN_SRC shell
shadow-opacity = 0.0;
#+END_SRC
The rest of the configuration [[https://github.com/HaoZeke/Dotfiles/blob/master/dotfiles/archLinux/.config/picom.conf][is here]]; and contains a lot more, mostly
pertaining to opacity and other pretty effects[fn:plugdots].
** Conclusion
Finally we have achieved the goal of having normal screen sharing capabilities;
as seen below:

#+DOWNLOADED: screenshot @ 2020-05-12 01:13:37
#+caption: Just in time to see an excellent pun
[[file:images/Conclusion/2020-05-12_01-13-37_screenshot.png]]

The struggle was real, though the cause was trivial, and really highlights the
need to always know your system packages. In this case, no doubt my students
would have preferred not having to suffer through the darkness of my
screen[fn:bojackplug]. This has been a rather trivial post, but one to keep in
mind none-the-less.

[fn:whatglitch] To be clear, none of the windows were the glitch. The issue was the darkened overlay
[fn:bojackplug] Though it might have also served as a metaphor for *darkness*
[fn:plugdots] The rest of [[https://github.com/HaoZeke/Dotfiles][my Dotfiles]], managed by the [[https://github.com/kobus-v-schoor/dotgit][excellent dotgit]] are also worth a look

[fn:archwikiplug] As always, the [[https://wiki.archlinux.org/index.php/Picom][ArchLinux Wiki]] is a great place for more information

* TODO Fancy Unicode XeTeX in Orgmode :@notes:tools:emacs:workflow:orgmode:
:PROPERTIES:
:EXPORT_FILE_NAME: fancy-unicode-orgmode
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:

\begin{align}
\nonumber W_{r\rightarrow\infty}=&-\int_{r}^{\infty}\!F\,\mathrm{d}y=-    \int_r^\infty \!     \dfrac{1}{4\pi \epsilon_0} \dfrac{q^2}{\alpha^2}     \dfrac{\alpha^3}{y^3}\left(1-    \dfrac{\alpha^2}    {y^2}\right)^{-2}\,\mathrm{d}y\\
=&-\dfrac{1}{4\pi \epsilon_0} \dfrac{q^2}{\alpha^2}\alpha^3     \underbrace{\int_r^\infty     \! y^{-3} \left(1-\dfrac{\alpha^2}    {y^2}\right)^{-2} \,\mathrm{d}y}_{I} \label{eq:WcondI}
\end{align}

* DONE LosslessCut, Zoom and an AMA for CS106A :@notes:teaching:cs106a:tools:
CLOSED: [2020-05-20 Wed 20:21]
:PROPERTIES:
:EXPORT_FILE_NAME: losslesscut-zoom-ama
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments false
:END:
** Background
I recently had the opportunity to take part in an AMA (ask me anything) session
for the CS106A students on Machine Learning for the Physical Sciences. This is a
post about the technical issues, and also includes a video if you read through.
** Zoom and LosslessCut
Zoom recordings are one of the nicer ways to deal with switching windows and
screen sharing, especially after fixing the [[Compton to Picom and Zoom Glitches][dark screen glitch]]. However, though
[[https://github.com/mifi/lossless-cut][LosslessCut]] works really well to get cut-points, exporting and merging the file
into one caused a bunch of glitches.
** Enter Handbrake
To not beat around the bush, the solution was to simply encode the Zoom
recording with [[https://handbrake.fr/][Handbrake]] before using LosslessCut[fn:whatsettings]. Since the
conversion takes a while, it is also neat to note that you can directly export
the cut points made with LosslessCut on the original video, then import them
onto the newly encoded file.
** Conclusions
I am not really sure how this will turn out, but it is a useful thing to keep in
mind. The introductory video turned out to be:

{{< youtube aOuqgyHHOK4 >}}

[fn:whatsettings] For me, the Vimeo Youtube HQ 1080p60 preset worked out well
* TODO FOSS Maintenance and Me :@personal:ramblings:explanations:thoughts:
Write about being the [[https://github.com/chriskempson/base16/issues/225#issuecomment-639739616][new maintainer]] for the base16-zathura thing and the AUR packages I maintain.
* DONE Nix with R and devtools :@programming:tools:nix:workflow:R:
CLOSED: [2020-06-06 Sat 05:49]
:PROPERTIES:
:EXPORT_FILE_NAME: nix-r-devtools
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
This post discusses briefly, the ~nix-shell~ environment for reproducible
programming. In particular, there is an emphasis on extensions for installing
and working with packages not in [[https://cran.r-project.org/web/packages/][CRAN]], i.e. packages off Github which are
normally installed with ~devtools~.
#+END_QUOTE

** Background
The entire [[https://nixos.org/][nix ecosystem]] is fantastic, and is the main packaging
system used by [[https://dseams.info][d-SEAMS]] as well.
Recently I began working through the [[https://xcelab.net/rm/statistical-rethinking/][excellent second edition]] of "Statistical
Rethinking" by [[https://twitter.com/rlmcelreath][Richard McElreath]][fn:whatwhy].

Unfortunately, the ~rethinking~ package which is a major component of the book
itself depends on the V8 engine for some reason. The reigning AUR[fn:whutaur]
package ([[https://aur.archlinux.org/packages/v8-r/][V8-r]]) broke with a [[https://aur.archlinux.org/packages/v8-r/#comment-749561][fun error message]] I couldn't be bothered to deal
with. Ominously, the [[https://pastebin.com/CbdCMZ8d][rest of the logs]] prominently featured ~Warning: Running
gclient on Python 3.~. Given that older ~python~ versions have been permanently
retired, this seemed like a bad thing to deal with[fn:sonothing]. In any case,
having weaned off non-nix dependency tools for ~python~ and friends, it seemed
strange to not do the same for R.

The standard installation for the package entails obtaining ~rstan~ (which is trivial with ~nixpkgs~) and then using:
#+BEGIN_SRC R
install.packages(c("coda","mvtnorm","devtools","loo","dagitty"))
library(devtools)
devtools::install_github("rmcelreath/rethinking")
#+END_SRC
We will break this down and work through this installation in Nix space.
** Nix and R
The standard approach to setting up a project ~shell.nix~ is simply by using the
~mkshell~ function. There are some common aspects to this workflow, with more
language specific details documented here. A simple first version might be:

#+BEGIN_SRC nix
let
    pkgs = import <nixpkgs> { };
in pkgs.mkShell {
    buildInputs = with pkgs; [
        zsh
        R
        rPackages.ggplot
        rPackages.data_table
    ];
    shellHook = ''
    echo "hello"
    '';
  LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux
    "${glibcLocales}/lib/locale/locale-archive";
}
#+END_SRC

Where we note that we can install CRAN packages as easily as regular packages
(like R), except for the fact that they are kept in a ~pkgs.rPackages~
environment, as opposed to ~pkgs~. This is actually a common convention most
languages with central repos. The most interesting thing to note is that,
similar to the convention for ~nix-python~ setups, packages with a dot in the
name will be converted to having an underscore, i.e. ~data.table~ ->
~data_table~.

However, for the rethinking package, and many others, there is no current CRAN
package, and so the ~rPackages~ approach fails.

The ~LOCALE_ARCHIVE~ needs to be set for Linux machines, and is required for
working with other packages.

** Nix-R and Devtools
To work with non-CRAN packages, we need to modify our package setup a little. We
will also simplify our file to split the ~pkgs~ and the ~r-pkgs~.
*** Naive Approach
The naive approach works by using the ~shellHook~ to set ~R_LIBS_USER~ to save
user packages per-project.
#+BEGIN_SRC nix
{ pkgs ? import <nixpkgs> { } }:
with pkgs;
let
  my-r-pkgs = rWrapper.override {
    packages = with rPackages; [
      ggplot2
      knitr
      rstan
      tidyverse
      V8
      dagitty
      coda
      mvtnorm
      shape
      Rcpp
      tidybayes
    ];
  };
in mkShell {
  buildInputs = = with pkgs;[ git glibcLocales openssl openssh curl wget ];
  inputsFrom = [ my-r-pkgs ];
  shellHook = ''
    mkdir -p "$(pwd)/_libs"
    export R_LIBS_USER="$(pwd)/_libs"
  '';
  GIT_SSL_CAINFO = "${cacert}/etc/ssl/certs/ca-bundle.crt";
  LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux
    "${glibcLocales}/lib/locale/locale-archive";
}
#+END_SRC

Note that here we will also need to set the ~GIT_SSL_CAINFO~ to prevent some
errors during the build process[fn:seenelsewhere].
*** Native Approach
The native approach essentially leverages the ~nix~ method for building ~R~
packages. This is the most reproducible of the lot, and also has the useful
property of storing the files in the ~nix-store~ so re-using packages across
different projects will not store, build or download the package again. The
values required can be calculated from ~nix-prefetch-git~ as follows:

#+BEGIN_SRC bash
nix-env -i nix-prefetch-git
nix-prefetch-git https://github.com/rmcelreath/rethinking.git
#+END_SRC

The crux of this approach is the following snippet[fn:defnfrm]:
#+BEGIN_SRC nix
(buildRPackage {
  name = "rethinking";
  src = fetchFromGitHub {
    owner = "rmcelreath";
    repo = "rethinking";
    rev = "d0978c7f8b6329b94efa2014658d750ae12b1fa2";
    sha256 = "1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0";
  };
  propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ];
 })
#+END_SRC

**** Project Shell
This formulation for some strange reason does not work from the shell or environment by default, but does work with ~nix-shell --run bash --pure~.
#+BEGIN_SRC nix
{ pkgs ? import <nixpkgs> { } }:
with pkgs;
let
  my-r-pkgs = rWrapper.override {
    packages = with rPackages; [
      ggplot2
      knitr
      rstan
      tidyverse
      V8
      dagitty
      coda
      mvtnorm
      shape
      Rcpp
      tidybayes
      (buildRPackage {
        name = "rethinking";
        src = fetchFromGitHub {
          owner = "rmcelreath";
          repo = "rethinking";
          rev = "d0978c7f8b6329b94efa2014658d750ae12b1fa2";
          sha256 = "1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0";
        };
        propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ];
      })
    ];
  };
in mkShell {
  buildInputs = with pkgs; [ git glibcLocales openssl which openssh curl wget my-r-pkgs ];
  shellHook = ''
    mkdir -p "$(pwd)/_libs"
    export R_LIBS_USER="$(pwd)/_libs"
    echo ${my-r-pkgs}/bin/R
  '';
  GIT_SSL_CAINFO = "${cacert}/etc/ssl/certs/ca-bundle.crt";
  LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux
    "${glibcLocales}/lib/locale/locale-archive";
}
#+END_SRC

The reason behind this is simply that ~rWrapper~ forms an extra package which
has lower precedence than the user profile ~R~, which is documented in more
detail here on the [[https://nixos.wiki/wiki/R][NixOS wiki]].
**** User Profile
This is a more general approach which defines the environment for R with all the
relevant libraries and is described in the [[https://nixos.org/nixpkgs/manual/#r][nixpkgs manual]]. The following code
should be placed in ~$HOME/.config/nixpkgs/config.nix~:

#+BEGIN_SRC nix
{
  packageOverrides = super:
    let self = super.pkgs;
    in {
      rEnv = super.rWrapper.override {
        packages = with self.rPackages; [
          ggplot2
          knitr
          tidyverse
          tidybayes
          (buildRPackage {
            name = "rethinking";
            src = self.fetchFromGitHub {
              owner = "rmcelreath";
              repo = "rethinking";
              rev = "d0978c7f8b6329b94efa2014658d750ae12b1fa2";
              sha256 = "1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0";
            };
            propagatedBuildInputs =
              [ coda MASS mvtnorm loo shape rstan dagitty ];
          })
        ];
      };
    };
}
#+END_SRC

This snippet allows us to use our ~R~ as follows:
#+BEGIN_SRC bash
# Install things
nix-env -f "<nixpkgs>" -iA rEnv
# Fix locale
export LOCALE_ARCHIVE="$(nix-build --no-out-link "<nixpkgs>" -A glibcLocales)/lib/locale/locale-archive"
# Profit
R
#+END_SRC

Note that in this method, on Linux systems, the locale problem has to be fixed
with the explicit export. This means that this should be used mostly with
project level environments, instead of populating the global shell RC files.

*Update:* There is [[Emacs for Nix-R][another post]] with methods to reload this configuration automatically
** Conclusions
Of the methods described, the most useful method for working with packages not
hosted on CRAN is through the user-profile, while the ~shell.nix~ method is
useful in conjunction, for managing various projects. So the ideal approach is
then to use the user profile for installing anything which normally uses
~devtools~ and then use ~shell.nix~ for the rest.

Note that if the [[Project Shell]] is used with a [[User Profile]] as described in the
next section, all packages defined there can be dropped and then the project
shell does not need to execute ~R~ by default. The simplified ~shell.nix~ is
then simply:

#+BEGIN_SRC nix
{ pkgs ? import <nixpkgs> { } }:
with pkgs;
let
  my-r-pkgs = rWrapper.override {
    packages = with rPackages; [
      ggplot2
    ];
  };
in mkShell {
  buildInputs = with pkgs;[ git glibcLocales openssl which openssh curl wget my-r-pkgs ];
  inputsFrom = [ my-r-pkgs ];
  shellHook = ''
    mkdir -p "$(pwd)/_libs"
    export R_LIBS_USER="$(pwd)/_libs"
  '';
  GIT_SSL_CAINFO = "${cacert}/etc/ssl/certs/ca-bundle.crt";
  LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux
    "${glibcLocales}/lib/locale/locale-archive";
}
#+END_SRC

The entire workflow for
~rethinking~ is [[Statistical Rethinking and Nix][continued here]].

[fn:whatwhy] As part of [[https://ugla.hi.is/kennsluskra/index.php?sid=&tab=nam&chapter=namskeid&id=71055920203][a summer course]] at the University of Iceland relating to their successful [[http://covid.hi.is/][COVID-19 model]]
[fn:whutaur] The [[https://wiki.archlinux.org/index.php/Arch_User_Repository][Arch User Repository]] is the port of first call for most ArchLinux users
[fn:sonothing] Though, like any good AUR user, I did post a bug report
[fn:seenelsewhere] This approach is also [[https://churchman.nl/tag/r/][discussed here]]
[fn:defnfrm] As discussed on [[https://github.com/NixOS/nixpkgs/issues/44290][this issue]], this [[https://stackoverflow.com/questions/55176609/how-to-install-r-and-packages-through-configuration-nix-and-how-to-add-packages][stackoverflow question]] and also [[https://github.com/rikhuijzer/nix-with-r/blob/master/default.nix][seen here]]

* DONE Statistical Rethinking and Nix :@programming:tools:nix:workflow:R:
CLOSED: [2020-06-07 Sun 04:24]
:PROPERTIES:
:EXPORT_FILE_NAME: rethinking-r-nix
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:

#+BEGIN_QUOTE
This post describes how to set up a transparent automated setup for reproducible
~R~ workflows using ~nixpkgs~, ~niv~, and ~lorri~. The explanatory example used
throughout the post is one of setting up the ~rethinking~ package and running
some examples
from the [[https://xcelab.net/rm/statistical-rethinking/][excellent second edition]] of "Statistical
Rethinking" by [[https://twitter.com/rlmcelreath][Richard McElreath]].
#+END_QUOTE

** Background
As detailed [[Nix with R and devtools][in an earlier post]][fn:wherehook], I had set up Nix to work with
non-CRAN packages. If the rest of this section is unclear, please refer back to [[Nix with R and devtools][the earlier post]].
*** Setup

For the remainder of the post, we will set up a basic project structure:
#+BEGIN_SRC bash
mkdir tryRnix/
#+END_SRC

#+RESULTS:

Now we will create a ~shell.nix~ as[fn:explaaiin]:
#+BEGIN_SRC  nix
# shell.nix
{ pkgs ? import <nixpkgs> { } }:
with pkgs;
let
  my-r-pkgs = rWrapper.override {
    packages = with rPackages; [
      ggplot2
      tidyverse
      tidybayes
      tidybayes.rethinking
      (buildRPackage {
        name = "rethinking";
        src = fetchFromGitHub {
          owner = "rmcelreath";
          repo = "rethinking";
          rev = "d0978c7f8b6329b94efa2014658d750ae12b1fa2";
          sha256 = "1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0";
        };
        propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ];
      })
    ];
  };
in mkShell {
  buildInputs = with pkgs; [ git glibcLocales openssl which openssh curl wget ];
  inputsFrom = [ my-r-pkgs ];
  shellHook = ''
    mkdir -p "$(pwd)/_libs"
    export R_LIBS_USER="$(pwd)/_libs"
  '';
  GIT_SSL_CAINFO = "${cacert}/etc/ssl/certs/ca-bundle.crt";
  LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux
    "${glibcLocales}/lib/locale/locale-archive";
}
#+END_SRC

So we have:
#+BEGIN_SRC bash :exports both
tree tryRnix
#+END_SRC

#+RESULTS:
| tryRnix |              |   |      |
| ‚îî‚îÄ‚îÄ     | shell.nix    |   |      |
| 0       | directories, | 1 | file |

*** Introspection
At this point:
- I was able to install packages (system and ~R~) arbitrarily
- I was able to use project specific folders
- Unlike ~npm~, ~pipenv~, ~poetry~, ~conda~ and friends, my system was not bloated by downloading and setting up the same packages every-time I used them in different projects

However, though this is a major step up from being chained to RStudio and my
system package manager, it is still perhaps not immediately obvious how this
workflow is reproducible. Admittedly, I have defined my packages in a nice
functional manner; but someone else might have a different upstream channel they
are tracking, and thus will have different packages. Indeed the only packages
which I could be sure of were the ~R~ packages I built from Github, since those
were tied to a hash. Finally, the setup described for each project is pretty
onerous, and it is not immediately clear how to leverage fantastic tools like
~direnv~ for working through this.
** Towards Reproducible Environments
The astute reader will have noticed that I mentioned that the ~R~ packages were
reproducible since they were tied to a *hash*, and might reasonable argue that
the entire Nix ecosystem is about *hashing* in the first place. Once we realize
that, the rest is relatively simple[fn:callbacktopost].
*** Niv and Pinning
[[https://github.com/nmattia/niv/][Niv]] essentially keeps track of the channel from which all the packages are installed. Setup is pretty minimal.
#+BEGIN_SRC bash
cd tryRnix/
nix-env -i niv
niv init
#+END_SRC

At this point, we have:
#+BEGIN_SRC bash :exports both
tree tryRnix
#+END_SRC

#+RESULTS:
| tryRnix |            |              |       |
| ‚îú‚îÄ‚îÄ     | nix        |              |       |
| ‚îÇ¬†¬†     | ‚îú‚îÄ‚îÄ        | sources.json |       |
| ‚îÇ¬†¬†     | ‚îî‚îÄ‚îÄ        | sources.nix  |       |
| ‚îî‚îÄ‚îÄ     | shell.nix  |              |       |
|         |            |              |       |
| 1       | directory, | 3            | files |

We will have to update our ~shell.nix~ to use the new sources.

#+BEGIN_SRC nix
let
  sources = import ./nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  stdenv = pkgs.stdenv;
  my-r-pkgs = pkgs.rWrapper.override {
    packages = with pkgs.rPackages; [
      ggplot2
      tidyverse
      tidybayes
    ];
  };
in pkgs.mkShell {
  buildInputs = with pkgs;[ git glibcLocales openssl which openssh curl wget my-r-pkgs ];
  shellHook = ''
    mkdir -p "$(pwd)/_libs"
    export R_LIBS_USER="$(pwd)/_libs"
  '';
  GIT_SSL_CAINFO = "${pkgs.cacert}/etc/ssl/certs/ca-bundle.crt";
  LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux
    "${pkgs.glibcLocales}/lib/locale/locale-archive";
}
#+END_SRC

We could inspect and edit these sources by hand, but it is much more convenient
to simply use ~niv~ again when we need to update these.

#+BEGIN_SRC bash
cd tryRnix/
niv update nixpkgs -b nixpkgs-unstable
#+END_SRC

At this stage we have a reproducible set of packages ready to use. However it is
still pretty annoying to have to go through the trouble of writing ~nix-shell~
and also waiting while it rebuilds when we change things.
*** Lorri and Direnv
[[Poetry and Direnv][In the past]], I have made my admiration for ~direnv~ very clear (especially for
~python-poetry~). However, though ~direnv~ does allow us to include arbitrary ~bash~ logic into our projects, it would be nice to have something which has some defaults for nix. Thankfully, the folks at TweagIO developed [[https://github.com/target/lorri][lorri]] to scratch that itch.

The basic setup is simple:

#+BEGIN_SRC bash
nix-env -i lorri
cd tryRnix/
lorri init
#+END_SRC

#+BEGIN_SRC bash :exports both
tree -a tryRnix/
#+END_SRC

#+RESULTS:
| tryRnix/ |            |              |       |
| ‚îú‚îÄ‚îÄ      | .envrc     |              |       |
| ‚îú‚îÄ‚îÄ      | nix        |              |       |
| ‚îÇ¬†¬†      | ‚îú‚îÄ‚îÄ        | sources.json |       |
| ‚îÇ¬†¬†      | ‚îî‚îÄ‚îÄ        | sources.nix  |       |
| ‚îî‚îÄ‚îÄ      | shell.nix  |              |       |
|          |            |              |       |
| 1        | directory, | 4            | files |

We can and should inspect the environment ~lorri~ wants us to load with ~direnv~ file:

#+BEGIN_SRC bash :exports both
cat tryRnix/.envrc
#+END_SRC

#+RESULTS:
: $(lorri direnv)

In and of itself that is not too descriptive, so we should run  that on our own first.

#+BEGIN_SRC bash
EVALUATION_ROOT="$HOME/.cache/lorri/gc_roots/407bd4df60fbda6e3a656c39f81c03c2/gc_root/shell_gc_root"

watch_file "/run/user/1000/lorri/daemon.socket"
watch_file "$EVALUATION_ROOT"

#!/usr/bin/env bash
# ^ shebang is unused as this file is sourced, but present for editor
# integration. Note: Direnv guarantees it *will* be parsed using bash.

function punt () {
    :
}

# move "origPreHook" "preHook" "$@";;
move() {
    srcvarname=$1 # example: varname might contain the string "origPATH"
    # drop off the source variable name
    shift

    destvarname=$1 # example: destvarname might contain the string "PATH"
    # drop off the destination variable name
    shift

    # like: export origPATH="...some-value..."
    export "${@?}";

    # set $original to the contents of the variable $srcvarname
    # refers to
    eval "$destvarname=\"${!srcvarname}\""

    # mark the destvarname as exported so direnv picks it up
    # (shellcheck: we do want to export the content of destvarname!)
    # shellcheck disable=SC2163
    export "$destvarname"

    # remove the export from above, ie: export origPATH...
    unset "$srcvarname"
}

function prepend() {
    varname=$1 # example: varname might contain the string "PATH"

    # drop off the varname
    shift

    separator=$1 # example: separator would usually be the string ":"

    # drop off the separator argument, so the remaining arguments
    # are the arguments to export
    shift

    # set $original to the contents of the the variable $varname
    # refers to
    original="${!varname}"

    # effectfully accept the new variable's contents
    export "${@?}";

    # re-set $varname's variable to the contents of varname's
    # reference, plus the current (updated on the export) contents.
    # however, exclude the ${separator} unless ${original} starts
    # with a value
    eval "$varname=${!varname}${original:+${separator}${original}}"
}

function append() {
    varname=$1 # example: varname might contain the string "PATH"

    # drop off the varname
    shift

    separator=$1 # example: separator would usually be the string ":"
    # drop off the separator argument, so the remaining arguments
    # are the arguments to export
    shift


    # set $original to the contents of the the variable $varname
    # refers to
    original="${!varname:-}"

    # effectfully accept the new variable's contents
    export "${@?}";

    # re-set $varname's variable to the contents of varname's
    # reference, plus the current (updated on the export) contents.
    # however, exclude the ${separator} unless ${original} starts
    # with a value
    eval "$varname=${original:+${original}${separator}}${!varname}"
}

varmap() {
    if [ -f "$EVALUATION_ROOT/varmap-v1" ]; then
        # Capture the name of the variable being set
        IFS="=" read -r -a cur_varname <<< "$1"

        # With IFS='' and the `read` delimiter being '', we achieve
        # splitting on \0 bytes while also preserving leading
        # whitespace:
        #
        #    bash-3.2$ printf ' <- leading space\0bar\0baz\0' \
        #                  | (while IFS='' read -d $'\0' -r x; do echo ">$x<"; done)
        #    > <- leading space<
        #    >bar<
        #    >baz<```
        while IFS='' read -r -d '' map_instruction \
           && IFS='' read -r -d '' map_variable \
           && IFS='' read -r -d '' map_separator; do
            unset IFS

            if [ "$map_variable" == "${cur_varname[0]}" ]; then
                if [ "$map_instruction" == "append" ]; then
                    append "$map_variable" "$map_separator" "$@"
                    return
                fi
            fi
        done < "$EVALUATION_ROOT/varmap-v1"
    fi


    export "${@?}"
}

function declare() {
    if [ "$1" == "-x" ]; then shift; fi

    # Some variables require special handling.
    #
    # - punt:    don't set the variable at all
    # - prepend: take the new value, and put it before the current value.
    case "$1" in
        # vars from: https://github.com/NixOS/nix/blob/92d08c02c84be34ec0df56ed718526c382845d1a/src/nix-build/nix-build.cc#L100
        "HOME="*) punt;;
        "USER="*) punt;;
        "LOGNAME="*) punt;;
        "DISPLAY="*) punt;;
        "PATH="*) prepend "PATH" ":" "$@";;
        "TERM="*) punt;;
        "IN_NIX_SHELL="*) punt;;
        "TZ="*) punt;;
        "PAGER="*) punt;;
        "NIX_BUILD_SHELL="*) punt;;
        "SHLVL="*) punt;;

        # vars from: https://github.com/NixOS/nix/blob/92d08c02c84be34ec0df56ed718526c382845d1a/src/nix-build/nix-build.cc#L385
        "TEMPDIR="*) punt;;
        "TMPDIR="*) punt;;
        "TEMP="*) punt;;
        "TMP="*) punt;;

        # vars from: https://github.com/NixOS/nix/blob/92d08c02c84be34ec0df56ed718526c382845d1a/src/nix-build/nix-build.cc#L421
        "NIX_ENFORCE_PURITY="*) punt;;

        # vars from: https://www.gnu.org/software/bash/manual/html_node/Bash-Variables.html (last checked: 2019-09-26)
        # reported in https://github.com/target/lorri/issues/153
        "OLDPWD="*) punt;;
        "PWD="*) punt;;
        "SHELL="*) punt;;

        # https://github.com/target/lorri/issues/97
        "preHook="*) punt;;
        "origPreHook="*) move "origPreHook" "preHook" "$@";;

        *) varmap "$@" ;;
    esac
}

export IN_NIX_SHELL=impure

if [ -f "$EVALUATION_ROOT/bash-export" ]; then
    # shellcheck disable=SC1090
    . "$EVALUATION_ROOT/bash-export"
elif [ -f "$EVALUATION_ROOT" ]; then
    # shellcheck disable=SC1090
    . "$EVALUATION_ROOT"
fi

unset declare

Jun 06 19:02:32.368 INFO lorri has not completed an evaluation for this project yet, expr: $HOME/Git/Github/WebDev/Mine/haozeke.github.io/content-org/tryRnix/shell.nix
Jun 06 19:02:32.368 WARN `lorri direnv` should be executed by direnv from within an `.envrc` file, expr: $HOME/Git/Github/WebDev/Mine/haozeke.github.io/content-org/tryRnix/shell.nix
#+END_SRC

Upon inspection, that seems to check out. So now we can enable this.

#+BEGIN_SRC bash
direnv allow
#+END_SRC

Additionally, we will need to stick to using a pure environment as much as
possible to prevent unexpected situations. So we set:

#+BEGIN_SRC bash
# .envrc
eval "$(lorri direnv)"
nix-shell --run bash --pure
#+END_SRC

There's still a catch though. We need to have ~lorri daemon~ running to make
sure the packages are built automatically without us having to exit the shell
and re-run things. We can [[https://github.com/target/lorri/blob/master/contrib/daemon.md][turn to the documentation]] for this. Essentially, we
need to have a user-level systemd socket file and service for ~lorri~.

#+BEGIN_SRC bash
# ~/.config/systemd/user/lorri.socket
[Unit]
Description=Socket for Lorri Daemon

[Socket]
ListenStream=%t/lorri/daemon.socket
RuntimeDirectory=lorri

[Install]
WantedBy=sockets.target
#+END_SRC

#+BEGIN_SRC bash
# ~/.config/systemd/user/lorri.service
[Unit]
Description=Lorri Daemon
Requires=lorri.socket
After=lorri.socket

[Service]
ExecStart=%h/.nix-profile/bin/lorri daemon
PrivateTmp=true
ProtectSystem=strict
ProtectHome=read-only
Restart=on-failure
#+END_SRC

With that we are finally ready to start working with our auto-managed,
reproducible environments.

#+BEGIN_SRC bash
systemctl --user daemon-reload && \
systemctl --user enable --now lorri.socket
#+END_SRC

** Rethinking
As promised, we will first test the setup to see that everything is working. Now
is also a good time to try the ~tidybayes.rethinking~ package. In order to use
it, we will need to define the ~rethinking~ package in a way so we can pass it
to the ~buildInputs~ for ~tidybayes.rethinking~. We will modify new ~shell.nix~
as follows:

#+BEGIN_SRC nix :tangle tryRnix/shell.nix
# shell.nix
let
  sources = import ./nix/sources.nix;
  pkgs = import sources.nixpkgs { };
  stdenv = pkgs.stdenv;
  rethinking = with pkgs.rPackages;
    buildRPackage {
      name = "rethinking";
      src = pkgs.fetchFromGitHub {
        owner = "rmcelreath";
        repo = "rethinking";
        rev = "d0978c7f8b6329b94efa2014658d750ae12b1fa2";
        sha256 = "1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0";
      };
      propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ];
    };
  tidybayes_rethinking = with pkgs.rPackages;
    buildRPackage {
      name = "tidybayes.rethinking";
      src = pkgs.fetchFromGitHub {
        owner = "mjskay";
        repo = "tidybayes.rethinking";
        rev = "df903c88f4f4320795a47c616eef24a690b433a4";
        sha256 = "1jl3189zdddmwm07z1mk58hcahirqrwx211ms0i1rzbx5y4zak0c";
      };
      propagatedBuildInputs =
        [ dplyr tibble rlang MASS tidybayes rethinking rstan ];
    };
  rEnv = pkgs.rWrapper.override {
    packages = with pkgs.rPackages; [
      ggplot2
      tidyverse
      tidybayes
      devtools
      modelr
      cowplot
      ggrepel
      RColorBrewer
      purrr
      forcats
      rstan
      rethinking
      tidybayes_rethinking
    ];
  };
in pkgs.mkShell {
  buildInputs = with pkgs; [ git glibcLocales which ];
  inputsFrom = [ rEnv ];
  shellHook = ''
    mkdir -p "$(pwd)/_libs"
    export R_LIBS_USER="$(pwd)/_libs"
  '';
  GIT_SSL_CAINFO = "${pkgs.cacert}/etc/ssl/certs/ca-bundle.crt";
  LOCALE_ARCHIVE = stdenv.lib.optionalString stdenv.isLinux
    "${pkgs.glibcLocales}/lib/locale/locale-archive";
}
#+END_SRC

The main thing to note here is that we need the output of the derivation we
create here, i.e. we need to use ~inputsFrom~ and NOT ~buildInputs~ for ~rEnv~.

Let us try to get a nice graphic for the conclusion.

#+BEGIN_SRC R :tangle tryRnix/tesPlot.R
library(magrittr)
library(dplyr)
library(purrr)
library(forcats)
library(tidyr)
library(modelr)
library(tidybayes)
library(tidybayes.rethinking)
library(ggplot2)
library(cowplot)
library(rstan)
library(rethinking)
library(ggrepel)
library(RColorBrewer)

theme_set(theme_tidybayes())
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())


set.seed(5)
n = 10
n_condition = 5
ABC =
  tibble(
    condition = factor(rep(c("A","B","C","D","E"), n)),
    response = rnorm(n * 5, c(0,1,2,1,-1), 0.5)
  )

mtcars_clean = mtcars %>%
  mutate(cyl = factor(cyl))

m_cyl = ulam(alist(
    cyl ~ dordlogit(phi, cutpoint),
    phi <- b_mpg*mpg,
    b_mpg ~ student_t(3, 0, 10),
    cutpoint ~ student_t(3, 0, 10)
  ),
  data = mtcars_clean,
  chains = 4,
  cores = parallel::detectCores(),
  iter = 2000
)

cutpoints = m_cyl %>%
  recover_types(mtcars_clean) %>%
  spread_draws(cutpoint[cyl])

# define the last cutpoint
last_cutpoint = tibble(
  .draw = 1:max(cutpoints$.draw),
  cyl = "8",
  cutpoint = Inf
)

cutpoints = bind_rows(cutpoints, last_cutpoint) %>%
  # define the previous cutpoint (cutpoint_{j-1})
  group_by(.draw) %>%
  arrange(cyl) %>%
  mutate(prev_cutpoint = lag(cutpoint, default = -Inf))

fitted_cyl_probs = mtcars_clean %>%
  data_grid(mpg = seq_range(mpg, n = 101)) %>%
  add_fitted_draws(m_cyl) %>%
  inner_join(cutpoints, by = ".draw") %>%
  mutate(`P(cyl | mpg)` =
    # this part is logit^-1(cutpoint_j - beta*x) - logit^-1(cutpoint_{j-1} - beta*x)
    plogis(cutpoint - .value) - plogis(prev_cutpoint - .value)
  )


data_plot = mtcars_clean %>%
  ggplot(aes(x = mpg, y = cyl, color = cyl)) +
  geom_point() +
  scale_color_brewer(palette = "Dark2", name = "cyl")

fit_plot = fitted_cyl_probs %>%
  ggplot(aes(x = mpg, y = `P(cyl | mpg)`, color = cyl)) +
  stat_lineribbon(aes(fill = cyl), alpha = 1/5) +
  scale_color_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2")

png(filename="../images/rethinking.png")
plot_grid(ncol = 1, align = "v",
  data_plot,
  fit_plot
)
dev.off
#+END_SRC

Finally we will run this in our environment.

#+BEGIN_SRC bash
Rscript tesPlot.R
#+END_SRC

[[file:images/rethinking.png]]

** Conclusions
This post was really more of an exploratory follow up to the previous post, and
does not really work in isolation. Then again, at this point everything seems to
have worked out well. ~R~ with Nix has finally become a truly viable combination
for any and every analysis under the sun. Some parts of the workflow are still a
bit janky, but will probably resolve themselves over time.

*Update:* There is [[Emacs for Nix-R][a final part]] detailing automated ways of reloading the system configuration

[fn:wherehook] My motivations were laid out in the [[Nix with R and devtools][aforementioned post]], and will not be repeated
[fn:explaaiin] For why these are the way they are see the this is written, see the [[Nix with R and devtools][aforementioned post]]
[fn:callbacktopost] Christine Dodrill [[https://christine.website/blog/how-i-start-nix-2020-03-08][has a great write up]] on using these tools as well
* DONE Emacs for Nix-R :@programming:tools:nix:workflow:R:emacs:
CLOSED: [2020-06-10 Wed 00:12]
:PROPERTIES:
:EXPORT_FILE_NAME: emacs-nix-r
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A short post on my current set-up for ~R~ with ~nixpkgs~ and ~emacs~ to
auto-compile my system configuration.
#+END_QUOTE
** Background
This is my third post on working with ~nixpkgs~ and ~R~.

- [[Nix with R and devtools][Part I]] covered ways of working effectively with ~R~ and ~nixpkgs~
- [[Statistical Rethinking and Nix][Part II]] dealt with composing dependent ~devtools~ packages in a per-package environment, with a focus on ~rethinking~ and ~tidybayes.rethinking~

This final part is about automating the system-wide configuration using ~emacs~.
Specifically ~doom-emacs~. Naturally, this is the most optimal way to work with
~nix~ packages as well.
*** System Configuration
After experimenting with a per-project layout, I decided to use the full system
configuration instead of the per-project layout. So I simply set:
#+BEGIN_SRC nix
# $HOME/.config/nixpkgs/config.nix
{
  packageOverrides = super:
    let
      self = super.pkgs;
      rethinking = with self.rPackages;
        buildRPackage {
          name = "rethinking";
          src = self.fetchFromGitHub {
            owner = "rmcelreath";
            repo = "rethinking";
            rev = "d0978c7f8b6329b94efa2014658d750ae12b1fa2";
            sha256 = "1qip6x3f6j9lmcmck6sjrj50a5azqfl6rfhp4fdj7ddabpb8n0z0";
          };
          propagatedBuildInputs = [ coda MASS mvtnorm loo shape rstan dagitty ];
        };
      tidybayes_rethinking = with self.rPackages;
        buildRPackage {
          name = "tidybayes.rethinking";
          src = self.fetchFromGitHub {
            owner = "mjskay";
            repo = "tidybayes.rethinking";
            rev = "df903c88f4f4320795a47c616eef24a690b433a4";
            sha256 = "1jl3189zdddmwm07z1mk58hcahirqrwx211ms0i1rzbx5y4zak0c";
          };
          propagatedBuildInputs =
            [ dplyr tibble rlang MASS tidybayes rethinking rstan ];
        };
    in {
      rEnv = super.rWrapper.override {
        packages = with self.rPackages; [
          tidyverse
          devtools
          modelr
          purrr
          forcats
          ####################
          # Machine Learning #
          ####################
          # MLR3
          mlr3
          mlr3viz
          mlr3learners
          mlr3pipelines
          # Plotting tools
          ggplot2
          cowplot
          ggrepel
          RColorBrewer
          # Stan Stuff
          rstan
          tidybayes
          # Text Utilities
          orgutils
          latex2exp
          kableExtra
          knitr
          data_table
          printr
          # Devtools Stuff
          rethinking
          tidybayes_rethinking
        ];
      };
    };
}
#+END_SRC
If any of these look strange, refer to the [[Nix with R and devtools][earlier posts]].
** Automation Pains
~direnv~, ~lorri~ and
~niv~ (the heroes of [[Statistical Rethinking and Nix][Part II]]) are not really useful for working with the system-wide configuration, but
an elegant solution still exists, which leverages ~firestarter~ and
~after-save-hooks~ in ~emacs~.
*** Firestarter
[[https://depp.brause.cc/firestarter/][Firestarter]] is my favorite method of working with shell commands after saving
things. My setup is simply:
#+begin_src emacs-lisp
; packages.el
(package! firestarter)
#+end_src
This is coupled with a simple configuration.
#+begin_src emacs-lisp
; config.el
(use-package! firestarter
  :ensure t
  :init
  (firestarter-mode)
  :config
  (setq firestarter-default-type t)
)
#+end_src
The default type corresponds to demanding the shell outupt for the commands.
*** Nix-R Stuff
To finalize this setup, we will need to modify our system configuration
slightly. For brevity, we simply note the following local variables.

#+BEGIN_SRC nix
# $HOME/.config/nixpkgs/config.nix
# Local Variables:
# firestarter: "nix-env -f '<nixpkgs>' -iA rEnv"
# firestarter-default-type: (quote failure)
# End:
#+END_SRC

The ~firestarter-default-type~ used here is to ensure that errors are displayed
in a buffer.

To check what is being installed (if anything) simply run:
#+BEGIN_SRC bash
nix-env -f "<nixpkgs>" -iA rEnv --dry-run
#+END_SRC
** Conclusion
This is my current setup. It works out better than most of my other attempts and
seems to be an optimal approach. The packages are versioned, everything is
automated, and I can reproduce changes across all my machines. Will stick with
this.

* DONE Temporary LaTeX Documents with Orgmode :@programming:tools:emacs:workflow:orgmode:
CLOSED: [2020-06-19 Fri 05:07]
:PROPERTIES:
:EXPORT_FILE_NAME: org-arb-tex
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A post on working with transient TeX templates in ~orgmode~ without modifying global configurations. This will also serve as a rudimentary introduction to TeX in ~orgmode~.
#+END_QUOTE
** Background
The sad reality of working in a field dominated by institutional actors which do not care for recognizing software development as a skill is that there are often a lot of ugly LaTeX templates[fn:thatsall]. In particular, often Universities have arbitrary LaTeX templates from the the dark days of 2010 something, which include gratuitous usage of say, ~natbib~ instead of ~biblatex~. In other situations, ~.cls~ files define separate document classes which are not covered by the ~orgmode~ defaults and need to be accounted for.
** Standard methods
Essentially for the exporter, the document is broken into[fn:halptex]:
- *document_class* :: This cannot be changed arbitrarily and [[https://orgmode.org/manual/LaTeX-specific-export-settings.html#LaTeX-specific-export-settings][has to be]] a valid element of ~org-latex-classes~
- *preamble* :: This section of the document is essentially everything before ~\begin{document}~ and after ~\documentclass{...}~
- *body* :: The rest of the document

We will briefly cover the standard methods of entering TeX in each of these sections, in reverse order since that is the direction in which the intuitive aspect decreases.
*** In-Body TeX
The method of writing TeX in ~orgmode~ for the document involves simply writing TeX directly, or wrapping the TeX markup in an export TeX block for font locking[fn:fontwhat]. Essentially, for a document snippet:
#+BEGIN_SRC latex :exports code
% #+BEGIN_SRC latex :exports code
\begin{align}
\pi(x) &= \sum_{n=1}^{\infty}\frac{\mu(n)}{n}\Pi(x^{\frac{1}{n}}) \\
       &= \Pi(x) -\frac{1}{2}\Pi(x^{\frac{1}{2}}) - \frac{1}{3}\Pi(x^{\frac{1}{3}}) - \frac{1}{5}\Pi(x^{\frac{1}{5}}) + \frac{1}{6} \Pi(x^{\frac{1}{6}}) -\cdots,
\end{align}
% #+END_SRC
#+END_SRC

Which will actually be rendered in a real document of course[fn:butwhatformula]:
# It is the M√∂bius inversion formula

\begin{align}
\pi(x) &= \sum_{n=1}^{\infty}\frac{\mu(n)}{n}\Pi(x^{\frac{1}{n}}) \\
       &= \Pi(x) -\frac{1}{2}\Pi(x^{\frac{1}{2}}) - \frac{1}{3}\Pi(x^{\frac{1}{3}}) - \frac{1}{5}\Pi(x^{\frac{1}{5}}) + \frac{1}{6} \Pi(x^{\frac{1}{6}}) -\cdots,
\end{align}

There is also the inline form of writing LaTeX with ~@@\sin{x}@@~ which is essentially $\sin{x}$.
*** Preamble
The main use of the preamble is to either add classes or modify class options for loaded packages like ~geometry~. Essentially, for ~orgmode~, anything prefixed with ~#+LATEX_HEADER:~ gets inserted in the preamble.
#+BEGIN_SRC org :exports code
#+LATEX_HEADER: \usepackage{amssymb,amsmath,MnSymbol}
#+LATEX_HEADER: \usepackage{unicode-math}
#+LATEX_HEADER: \usepackage{mathtools}
#+END_SRC
For larger documents, this gets quite annoying for loading packages. We will demonstrate a more aesthetically pleasant form later in this post.
*** LaTeX classes
Working with document classes is the least intuitive of all TeX manipulations, because for some reason, ~#+LATEX_CLASS:~ only accepts values defined in ~org-latex-classes~.

The standard approach to extending the ~orgmode~ TeX backend is to add lines like the following in ~init.el~ or, in my case[fn:whatwhy], ~config.org~:
#+BEGIN_SRC emacs-lisp
(add-to-list 'org-latex-classes
             '("koma-article" "\\documentclass{scrartcl}"
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\paragraph{%s}" . "\\paragraph*{%s}")
               ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
#+END_SRC

This is alright for often used classes like the ~koma-‚ãÜ~ family of LaTeX document-classes, but it is hardly ideal for one-off TeX templates which are meant for say, grant proposals[fn:grantwhat].
** Elisp to the rescue
The core idea is quite simple.
#+BEGIN_QUOTE
Since ~orgmode~ files are literate documents, and ~emacs~ is self-documenting and completely programmable, it should be possible to execute code to deterministically set the state of ~emacs~ before exporting the document to TeX.
#+END_QUOTE
Practically this has a few moving parts. In the following sections, assume that we have a ~.cls~ file which defines a document-class ~foo~ with a bunch of packages which conflict with our global configuration.
*** Adding Document Classes
Instead of adding the code snippet to our global configuration, we will now add it to the document directly with the comments indicating the appropriate environment.
#+BEGIN_SRC emacs-lisp :exports code  :results none :eval never
;; #+BEGIN_SRC emacs-lisp :exports none  :results none :eval always
(add-to-list 'org-latex-classes
             '("foo" "\\documentclass{foo}"
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\paragraph{%s}" . "\\paragraph*{%s}")
               ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
;; #+END_SRC
#+END_SRC

Where the header arguments simply ensure that the code and result do not show up in the document, and that the chunk is always evaluated by ~org-babel~.
*** Ensuring Purity
Since we want to stick to an external template defined in ~foo~ *and no other packages* we will need to clear the defaults we lovingly set globally for our convenience.
#+BEGIN_SRC emacs-lisp :exports code  :results none :eval never
;; #+BEGIN_SRC emacs-lisp :exports none  :results none :eval always
(setq org-latex-packages-alist 'nil)
;; #+END_SRC
#+END_SRC
*** Pretty Packages
For packages we really would like to add, we can now leverage the ~elisp~ code instead of the ugly ~#+LATEX_HEADER:~ lines.
#+BEGIN_SRC emacs-lisp :exports code  :results none :eval never
;; #+BEGIN_SRC emacs-lisp :exports none  :results none :eval always
(setq org-latex-default-packages-alist
  '(("utf8" "inputenc"  t)
    ("normalem" "ulem"  t)
    (""     "mathtools"   t)
    ))
;; #+END_SRC
#+END_SRC
Note that for setting options, we will still need to use the ~#+LATEX_HEADER:~ syntax.
*** Automating With Hooks
At this stage, we have a chunk of ~elisp~ we can manually evaluate with ~org-babel~ before exporting with ~org-latex-export-to-pdf~ or ~org-latex-export-to-latex~. However, this can get old quickly, so we will instead have a ~before-save-hook~ to do this for us.
#+BEGIN_SRC org :exports code
# Local Variables:
# before-save-hook: org-babel-execute-buffer
# End:
#+END_SRC
**** Bonus Hook
In [[https://dotdoom.rgoswami.me/config.html#org13eed8a][my own configuration]], I have a function defined for an ~after-save-hook~ which generates the TeX file without having me deal with it. For a per-file configuration of this, or globally, the ~elisp~ is:
#+BEGIN_SRC emacs-lisp :exports code
(defun haozeke/org-save-and-export-latex ()
  (if (eq major-mode 'org-mode)
    (org-latex-export-to-latex)))
#+END_SRC
This indirection is required to call the function as a ~hook~. Now this can be used as:
#+BEGIN_SRC org :exports code
# Local Variables:
# after-save-hook: haozeke/org-save-and-export-latex
# End:
#+END_SRC
** Conclusions
The entire file would look something like this (the ~elisp~ can be anywhere in the ~orgmode~ file):
#+BEGIN_SRC emacs-lisp :exports code  :results none :eval never
;; #+BEGIN_SRC emacs-lisp :exports none  :results none :eval always
(add-to-list 'org-latex-classes
             '("foo" "\\documentclass{foo}"
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\paragraph{%s}" . "\\paragraph*{%s}")
               ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
(setq org-latex-packages-alist 'nil)
(setq org-latex-default-packages-alist
  '(("utf8" "inputenc"  t)
    (""     "minted"   t)
    (""     "rotating"  nil)
    ("normalem" "ulem"  t)
    (""     "mathtools"   t)
    ))
;; #+END_SRC
#+END_SRC
#+BEGIN_SRC org :exports code
,#+TITLE: Something
,#+AUTHOR: Rohit Goswami
,#+OPTIONS: toc:nil \n:nil
,#+STARTUP: fninline
,#+LATEX_COMPILER: xelatex
,#+LATEX_CLASS: foo
,#+LATEX_HEADER: \setlength\parindent{0pt}
,#+LATEX_HEADER: \addbibresource{whatever.bib}

Blah blah document $\sin{x}$ stuff

# Local Variables:
# before-save-hook: org-babel-execute-buffer
# after-save-hook: haozeke/org-save-and-export-latex
# End:
#+END_SRC
This method could be extended to essentially resetting all ~emacs~ variables on a per-file basis (without ~file-local-variables~ and ~dir-local-variables~) or to potentially execute any ~elisp~ to make ~emacs~ do things, though I cannot really think of another realistic use-case. The method presented here is really general enough to work with any arbitrary LaTeX ~.cls~ file or other draconian measures.

[fn:thatsall] Of course there are more issues stemming from this toxic practice, but that's for another rant
[fn:whatwhy] I use [[https://github.com/hlissner/doom-emacs/][doom-emacs]] with my own [[https://dotdoom.rgoswami.me][literate configuration]]
[fn:fontwhat] Or syntax highlighting for most people
[fn:halptex] For more on document structure in TeX [[https://en.wikibooks.org/wiki/LaTeX/Document_Structure][read the wikibook]]
[fn:grantwhat] I haven't seen a grant proposal template I'd like to store for later, *ever*
[fn:butwhatformula] Props to anyone who recognizes that formula
* TODO Computational Science and Math
One of the earliest memories of feedback I recall was about my handwriting. To this day it remains an ugly scrawl, with legibility tending to nil in cursive.

* DONE LineageOS Maintainer Appreciation :@personal:ramblings:thoughts:
CLOSED: [2020-06-28 Sun 18:35]
:PROPERTIES:
:EXPORT_FILE_NAME: linos-maintainer-appre
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false :comments true
:END:
#+BEGIN_QUOTE
A post on a surprisingly heartwarming community appreciation effort.
#+END_QUOTE

** Background
As probably anyone who has asked me about my programming experience has heard, my first real foray into the FOSS community was being a [[https://forum.xda-developers.com/member.php?u=1964056][LineageOS co-maintainer]] (as HaoZeke) for the Xperia Z5 Dual. I haven't thought about the community all that much for a few years, mostly since XDA became pretty toxic, and Android development just got, less exciting.
** The Email
I recieved two of these from different accounts:
#+BEGIN_QUOTE
Thank you so much for your contribution to making my phone the phone it is!

Have a great day!
#+END_QUOTE

[[file:images/lineageOS.jpg]]
** Conclusion
This was completely unexpected, and really made my day. In reterospect this seems like something which should be made more explicit, more often.

* DONE Analytics II: Goat to Clicky :@notes:tools:rationale:workflow:ideas:
CLOSED: [2020-07-06 Mon 23:09]
:PROPERTIES:
:EXPORT_FILE_NAME: goat-clicky
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A follow up to [[Analytics: Google to Goat][my earlier post on analytics]], and on migrating from Goat Counter to Clicky.
#+END_QUOTE

** Background
A few days ago I recieved the following email:
#+BEGIN_QUOTE
Hi there!

I made some changes to the GoatCounter plans/pricing:

1. GoatCounter now has a "Starter" plan, this is ‚Ç¨5/month, limited to 100k pageviews/month, comes with a custom domain, and allow commercial use. This is mostly the same as the "personal plus" plan there was before, except that it allows commercial use. If you had a "personal plus" for a custom domain before then you now have a Starter plan.

2. Starting on August 1st the data retention will be limited for the Free and Starter plans: the Free plan will be limited to 6 months, the Starter plan to 12 months, and the business plans remain unlimited. There is an export feature if you wish to retain your old pageviews.

Some background on this:

There seems to be a gap between "free for personal use" and "‚Ç¨15/month for commercial use". I've gotten quite a bit of feedback of small (potential) commercial users who just run a small website, where ‚Ç¨15/month really is prohibitively expensive.

The entire idea behind making it free for personal use is that I'd like GoatCounter to be usable by as many people as possible, while also ensuring commercial users pay their fair share. Redistributing software is free, but developing it is not.

The general thinking is that larger businesses with several employees (who can easily afford ‚Ç¨15/month) will have more than 100k pageviews/month, whereas for most startups and the like 100k should be more than enough.

The entire thing is a bit of a balancing act üòÖ I may tweak the pricing further in the future based on additional experience and feedback.

As for the data retention: the biggest issue here is that some not-especially-active sites have had short bursts of millions of pageviews in a short time because they wrote or made something that got widely shared.

The hits/months limit isn't strictly enforced because I don't want to tell people to get a plan just because they wrote a popular article that got to the front page of HN, Reddit, Twitter, etc, and GoatCounter has no problem handling these levels of pageviews, so that's all fine.

But on the other hand, a million pageviews currently takes up about 400M of disk space including backups (although this could probably be reduced a bit with a more clever backup strategy). Disk space is pretty cheap, but it does add up.

It also means more effort on scaling GoatCounter; limiting the data retention is an easy way to reduce the pressure on this. It also gives people a bit more incentive to get a plan üòÖ

As always, self-hosting isn't affected by any of this. This just applies to the goatcounter.com service.

Feel free to let me know if you've got any questions or feedback.

Cheers,
Martin
#+END_QUOTE
** Personal Repercussions
I should point out that I unequivocally support Martin's decision. It is fair and equitable. That said, continuity is super important to me. I've mentioned earlier that for me, the daily limit is not much of an issue but I do like to look back at my collective history.
** Goat Counter
[[https://www.goatcounter.com/][Goat Counter]] is still definitely my go-to option for both self-hosting and shelling out, if the 15 euro fee is acceptable. Honestly, the best option is probably opening a PR or making a tool to aggregate metrics offline, since it is still possible to export the data. The major drawback is the six month window.
** Clicky
[[https://clicky.com/][Clicky]] is pretty great, and they have a good example of [[https://clicky.com/compare/google][what works out in their favor compared to Google Analytics]].
*** Pros
- Free tier has no time limit
- Has a nice dark theme
*** Cons
- Not open source
- Capped at 3K daily views
- Blocked by some VPN providers (Windscribe)
** Conclusions
It is unfortunate to have had to move, since this does imply losing the past eight months of metrics. Eventually I might even go back to steady dependable Google Analytics. Until this, Clicky will do.
* DONE Multiple Monitors with Touchscreens :@notes:tools:workflow:
CLOSED: [2020-07-11 Sat 22:45]
:PROPERTIES:
:EXPORT_FILE_NAME: multi-monitor-touch
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true :comments true
:END:
#+BEGIN_QUOTE
A short tutorial post on multiple screens for laptops with touch-support and ArchLinux. Also evolved into a long rant, with an Easter egg.
#+END_QUOTE
** Background
Of late, I have been attempting to move away from paper, for environmental reasons[fn:environ]. Years of touch typing in Colemak ([[Switching to Colemak][rationale]], [[Refactoring Dotfiles For Colemak][config changes]]) and a very customized [[https://dotdoom.rgoswami.me][Emacs setup]] (including [[Using Mathematica with Orgmode][mathematica]], [[Temporary LaTeX Documents with Orgmode][temporary latex templates]], [[Emacs for Nix-R][Nix]], and [[An Orgmode Note Workflow][org-roam annotations]]) have more or less kept me away from analog devices. One might even argue that my current website is essentially a set of tricks to move my life into ~orgmode~ completely.

However, there are still a few things I cannot do without a pointing device (and some kind of canvas). Scrawl squiggly lines on papers I'm reviewing. That and, scrawl weird symbols which don't actually follow a coherent mathematical notation but might be later written up in latex to prove a point. Also, and I haven't mastered any of the drawing systems (like [[https://en.wikibooks.org/wiki/LaTeX/PGF/TikZ][Tikz]]) yet, so for squiggly charts I rely on [[https://jamboard.google.com/][Jamboard]] (while teaching) and [[https://github.com/xournalpp/xournalpp][Xournal++]] for collaborations.

I also happen to have a [[https://www.notebookcheck.net/Lenovo-ThinkPad-X380-Yoga-i5-8250U-FHD-Convertible-Review.316442.0.html][ThinkPad X380]] (try ~sudo dmidecode -t system | grep Version~) which has an in-built stylus, and since Linux support for touchscreens from 2018 is known to be incredible, I coupled this with the [[https://www.lenovo.com/us/en/thinkvisionM14/][ThinkVision M14]] as a second screen.
** X-Windows and X-ternal Screens

We will define two separate solutions:
- [[https://github.com/Ventto/mons][mons]] :: Using arbitrary external monitors
- [[https://github.com/phillipberndt/autorandr][autorandr]] :: Setting up profiles for specific monitors

Finally we will leverage both to ensure a constant touchscreen area.

*** Autorandr
I use the [[https://github.com/phillipberndt/autorandr][python rewrite]] simply because that's the one which is in the [[https://www.archlinux.org/packages/community/any/autorandr/][ArchLinux community repo]]. To be honest, I came across this before I (re-)discovered ~mons~. The most relevant aspect of ~autorandr~ is using complicated configurations for different monitors, but it also does a mean job of running generic scripts as ~postswitch~ and ~prefix~ scripts.

*** Mons
~xrandr~ is awesome. Unfortunately, more often than not, I forget the commands to interact with it appropriately.
[[https://github.com/Ventto/mons][mons]] does my dirty work for me[fn:monsgood].

#+BEGIN_SRC bash
# -e is extend
mons -e left
# puts screen on the left
#+END_SRC

That and the similar ~right~ option, covers around 99% of all possible dual screen use-cases.

** Constant Touch

The problem is that by default, the entire combined screen area is assumed to be touch-enabled, which essentially means an awkward area of the screen which is dead to all input (since it doesn't exist). The key insight is that I never have more touch-enabled surfaces than my default screen, no matter how many extended screens are present. So the solution is:

#+BEGIN_QUOTE
Make sure the touch area is constant.
#+END_QUOTE

We need to figure out what the touch input devices are:

#+BEGIN_SRC bash :exports both
xinput --list
#+END_SRC

#+RESULTS:
| ‚é° Virtual core pointer                             | id=2  | [master pointer  (3)] |
| ‚éú   ‚Ü≥ Virtual core XTEST pointer                   | id=4  | [slave  pointer  (2)] |
| ‚éú   ‚Ü≥ Wacom Pen and multitouch sensor Finger touch | id=10 | [slave  pointer  (2)] |
| ‚éú   ‚Ü≥ Wacom Pen and multitouch sensor Pen stylus   | id=11 | [slave  pointer  (2)] |
| ‚éú   ‚Ü≥ ETPS/2 Elantech TrackPoint                   | id=14 | [slave  pointer  (2)] |
| ‚éú   ‚Ü≥ ETPS/2 Elantech Touchpad                     | id=15 | [slave  pointer  (2)] |
| ‚éú   ‚Ü≥ Wacom Pen and multitouch sensor Pen eraser   | id=17 | [slave  pointer  (2)] |
| ‚é£ Virtual core keyboard                            | id=3  | [master keyboard (2)] |
| ‚Ü≥ Virtual core XTEST keyboard                      | id=5  | [slave  keyboard (3)] |
| ‚Ü≥ Power Button                                     | id=6  | [slave  keyboard (3)] |
| ‚Ü≥ Video Bus                                        | id=7  | [slave  keyboard (3)] |
| ‚Ü≥ Power Button                                     | id=8  | [slave  keyboard (3)] |
| ‚Ü≥ Sleep Button                                     | id=9  | [slave  keyboard (3)] |
| ‚Ü≥ Integrated Camera: Integrated C                  | id=12 | [slave  keyboard (3)] |
| ‚Ü≥ AT Translated Set 2 keyboard                     | id=13 | [slave  keyboard (3)] |
| ‚Ü≥ ThinkPad Extra Buttons                           | id=16 | [slave  keyboard (3)] |

At this point we will leverage ~autorandr~ to ensure that these devices are mapped to the primary (touch-enabled) screen with a ~postswitch~ script. This ~postswitch~ script needs to be:

#+BEGIN_SRC bash
#!/bin/sh
# .config/autorandr/postswitch
xinput --map-to-output 'Wacom Pen and multitouch sensor Finger touch' eDP1
xinput --map-to-output 'Wacom Pen and multitouch sensor Pen stylus' eDP1
xinput --map-to-output 'Wacom Pen and multitouch sensor Pen eraser' eDP1
notify-send "Screen configuration changed"
#+END_SRC

The last line of course is really more of an informative boast.

At this stage, we have the ability to set the touchscreens up by informing ~autorandr~ that our configuration has changed, through the command line for example:
#+BEGIN_SRC bash
autorandr --change
#+END_SRC

** Automatic Touch Configuration

Running a command manually on-change is the sort of thing which makes people think Linux is hard or un-intuitive. So we will instead make use of the incredibly powerful ~systemd~ framework for handling events.

Essentially, we combine our existing workflow with ~autorandr-launcher~ [[https://github.com/smac89/autorandr-launcher][from here]], and then set it all up as follows:

#+BEGIN_SRC bash
git clone https://github.com/smac89/autorandr-launcher.git
cd autorandr-launcher
sudo make install
sudo systemctl--user enable autorandr_launcher.service
#+END_SRC

** Conclusion
We now have a setup which ensures that the touch enabled area is constant, without any explicit manual interventions for when devices are added or removed. There isn't much else to say about this workflow here. Separate posts can deal with how exactly I meld Zotero, ~org-roam~ and Xournal++ to wreak havoc on all kinds of documents. So, in-lieu of a conclusion, behold a recent scribble with this setup:

#+caption: From the planning of [[http://www.wavelf.org/ij6TzydE3kTSF8cwwIUj][this voluntary course]]
file:images/myXournalScrib.png

[fn:environ] Also because paper is difficult to keep track of and is essentially the antithesis of a computer oriented workflow.
[fn:monsgood] I actually planned a whole post called "An Ode to Mons", when I first found out about it.

* TODO On Writing Grant Applications
